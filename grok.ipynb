{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86f7f08-bd1b-47a0-8142-2f2fcd8d4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Collecting ccxt\n",
      "  Downloading ccxt-4.4.71-py2.py3-none-any.whl.metadata (131 kB)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.12/dist-packages (from ccxt) (75.8.2)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.12/dist-packages (from ccxt) (2025.1.31)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.12/dist-packages (from ccxt) (2.32.3)\n",
      "Collecting cryptography>=2.6.1 (from ccxt)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from ccxt) (4.12.2)\n",
      "INFO: pip is looking at multiple versions of ccxt to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ccxt\n",
      "  Downloading ccxt-4.4.70-py2.py3-none-any.whl.metadata (131 kB)\n",
      "  Downloading ccxt-4.4.69-py2.py3-none-any.whl.metadata (131 kB)\n",
      "  Downloading ccxt-4.4.68-py2.py3-none-any.whl.metadata (131 kB)\n",
      "  Downloading ccxt-4.4.67-py2.py3-none-any.whl.metadata (131 kB)\n",
      "  Downloading ccxt-4.4.65-py2.py3-none-any.whl.metadata (129 kB)\n",
      "  Downloading ccxt-4.4.64-py2.py3-none-any.whl.metadata (130 kB)\n",
      "  Downloading ccxt-4.4.63-py2.py3-none-any.whl.metadata (130 kB)\n",
      "INFO: pip is still looking at multiple versions of ccxt to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading ccxt-4.4.62-py2.py3-none-any.whl.metadata (130 kB)\n",
      "  Downloading ccxt-4.4.61-py2.py3-none-any.whl.metadata (130 kB)\n",
      "  Downloading ccxt-4.4.60-py2.py3-none-any.whl.metadata (130 kB)\n",
      "  Downloading ccxt-4.4.59-py2.py3-none-any.whl.metadata (130 kB)\n",
      "  Downloading ccxt-4.4.58-py2.py3-none-any.whl.metadata (133 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading ccxt-4.4.57-py2.py3-none-any.whl.metadata (133 kB)\n",
      "  Downloading ccxt-4.4.53-py2.py3-none-any.whl.metadata (133 kB)\n",
      "  Downloading ccxt-4.4.52-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.51-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.50-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.49-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.48-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.47-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.46-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.45-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.44-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.43-py2.py3-none-any.whl.metadata (117 kB)\n",
      "  Downloading ccxt-4.4.42-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.41-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.40-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.39-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.38-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.37-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.36-py2.py3-none-any.whl.metadata (116 kB)\n",
      "  Downloading ccxt-4.4.35-py2.py3-none-any.whl.metadata (115 kB)\n",
      "  Downloading ccxt-4.4.34-py2.py3-none-any.whl.metadata (115 kB)\n",
      "  Downloading ccxt-4.4.33-py2.py3-none-any.whl.metadata (115 kB)\n",
      "  Downloading ccxt-4.4.32-py2.py3-none-any.whl.metadata (115 kB)\n",
      "  Downloading ccxt-4.4.31-py2.py3-none-any.whl.metadata (114 kB)\n",
      "  Downloading ccxt-4.4.30-py2.py3-none-any.whl.metadata (114 kB)\n",
      "Requirement already satisfied: aiohttp>=3.8 in /usr/local/lib/python3.12/dist-packages (from ccxt) (3.11.13)\n",
      "Collecting aiodns>=1.1.1 (from ccxt)\n",
      "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.12/dist-packages (from ccxt) (1.18.3)\n",
      "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt)\n",
      "  Downloading pycares-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8->ccxt) (0.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.6.1->ccxt) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->ccxt) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->ccxt) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->ccxt) (2.0.7)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt) (2.22)\n",
      "Downloading ccxt-4.4.30-py2.py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycares-4.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (289 kB)\n",
      "Installing collected packages: pycares, cryptography, aiodns, ccxt\n",
      "Successfully installed aiodns-3.2.0 ccxt-4.4.30 cryptography-44.0.2 pycares-4.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ccxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a8b6f3-5fee-45c0-ae6b-d0d97448d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 1000 свечей до 2018-01-30 16:00:00.001000\n",
      "Загружено 1000 свечей до 2018-07-18 00:00:00.001000\n",
      "Загружено 1000 свечей до 2018-12-31 20:00:00.001000\n",
      "Загружено 1000 свечей до 2019-06-17 00:00:00.001000\n",
      "Загружено 1000 свечей до 2019-11-30 20:00:00.001000\n",
      "Загружено 1000 свечей до 2020-05-15 16:00:00.001000\n",
      "Загружено 1000 свечей до 2020-10-29 08:00:00.001000\n",
      "Загружено 1000 свечей до 2021-04-14 00:00:00.001000\n",
      "Загружено 1000 свечей до 2021-09-27 16:00:00.001000\n",
      "Загружено 1000 свечей до 2022-03-13 08:00:00.001000\n",
      "Загружено 1000 свечей до 2022-08-27 00:00:00.001000\n",
      "Загружено 1000 свечей до 2023-02-09 16:00:00.001000\n",
      "Загружено 1000 свечей до 2023-07-26 08:00:00.001000\n",
      "Загружено 1000 свечей до 2024-01-09 00:00:00.001000\n",
      "Загружено 1000 свечей до 2024-06-23 16:00:00.001000\n",
      "Загружено 1000 свечей до 2024-12-07 08:00:00.001000\n",
      "Загружено 685 свечей до 2025-03-31 12:00:00.001000\n",
      "Данные сохранены в 'btc_usdt_4h_full.csv'. Всего строк: 16685\n"
     ]
    }
   ],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Инициализация биржи\n",
    "exchange = ccxt.binance()\n",
    "symbol = 'BTC/USDT'\n",
    "timeframe = '4h'\n",
    "since = exchange.parse8601('2017-08-17T00:00:00Z')  # Самая ранняя дата для BTC/USDT на Binance\n",
    "\n",
    "# Список для хранения всех данных\n",
    "all_ohlcv = []\n",
    "\n",
    "# Текущая временная метка в миллисекундах\n",
    "current_time = int(time.time() * 1000)  # Текущая дата в миллисекундах\n",
    "\n",
    "# Ограничение на количество свечей за запрос (Binance обычно возвращает до 500 или 1000)\n",
    "limit = 1000\n",
    "\n",
    "# Загрузка данных с пагинацией\n",
    "while since < current_time:\n",
    "    try:\n",
    "        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since, limit)\n",
    "        if not ohlcv:\n",
    "            break  # Если данных больше нет, выходим\n",
    "        all_ohlcv.extend(ohlcv)\n",
    "        since = ohlcv[-1][0] + 1  # Обновляем since до следующей свечи\n",
    "        print(f\"Загружено {len(ohlcv)} свечей до {pd.to_datetime(since, unit='ms')}\")\n",
    "        time.sleep(1)  # Задержка, чтобы не превысить лимит запросов API\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        break\n",
    "\n",
    "# Преобразование в DataFrame\n",
    "df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Удаление дубликатов, если есть\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "# Сохранение в CSV\n",
    "df.to_csv('btc_usdt_4h_full.csv')\n",
    "print(f\"Данные сохранены в 'btc_usdt_4h_full.csv'. Всего строк: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9028186a-30d0-4001-b529-de4199a0fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Collecting pandas_ta\n",
      "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pandas_ta) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas_ta) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas_ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas_ta) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas_ta) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->pandas_ta) (1.16.0)\n",
      "Building wheels for collected packages: pandas_ta\n",
      "  Building wheel for pandas_ta (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for pandas_ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218986 sha256=f5a1bfe568f87d23be1220ab99f00b740edff7ac85db3cacdc788b1f315e048f\n",
      "  Stored in directory: /root/.cache/pip/wheels/fd/ed/18/2a12fd1b7906c63efca6accb351929f2c7f6bbc674e1c0ba5d\n",
      "Successfully built pandas_ta\n",
      "Installing collected packages: pandas_ta\n",
      "Successfully installed pandas_ta-0.3.14b0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb898fba-8398-45c6-a912-c0118520aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157/429230575.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ebsw_result = ta.ebsw(df['close'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "\n",
    "# Загрузка данных\n",
    "df = pd.read_csv('btc_usdt_4h_full.csv', index_col='timestamp', parse_dates=True)\n",
    "\n",
    "# Differencing\n",
    "for k in range(1, 6):\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        df[f'{col}_diff_{k}'] = df[col] / df[col].shift(k)\n",
    "\n",
    "# EBSW\n",
    "ebsw_result = ta.ebsw(df['close'])\n",
    "df['ebsw'] = ebsw_result\n",
    "\n",
    "# Chaikin Money Flow\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "\n",
    "# VWAP\n",
    "df['tp'] = (df['low'] + df['high'] + df['close']) / 3\n",
    "df['vwap'] = (df['tp'] * df['volume']).cumsum() / df['volume'].cumsum()\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "\n",
    "# Other Ratios\n",
    "df['high_low'] = df['high'] / df['low']\n",
    "df['high_open'] = df['high'] / df['open']\n",
    "df['close_open'] = df['close'] / df['open']\n",
    "df['low_open'] = df['low'] / df['open']\n",
    "\n",
    "# Time Information\n",
    "df['hour'] = df.index.hour\n",
    "df['day'] = df.index.day\n",
    "df['month'] = df.index.month\n",
    "\n",
    "# Удаляем строки с NaN\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "\n",
    "def classify(u, v):\n",
    "    if u >= 0.0075:\n",
    "        return 0  # c0\n",
    "    elif v <= -0.0075:\n",
    "        return 1  # c1\n",
    "    else:\n",
    "        return 2  # c2\n",
    "\n",
    "df['target'] = [classify(u, v) for u, v in zip(df['u_t1'], df['v_t1'])]\n",
    "df.dropna(inplace=True)  # Удаляем последнюю строку\n",
    "\n",
    "# Сохранение обработанных данных\n",
    "df.to_csv('btc_usdt_4h_with_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3d0977-90ca-4c45-a89f-711554d1ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.2.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.14.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.26a0+c5e1555-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabbdb81-72c2-453b-9f66-d24db2a44bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143/607870151.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение y_train: [6147 2598  786]\n",
      "Распределение y_val: [167  88   9]\n",
      "Распределение y_test: [245 125  39]\n",
      "Веса классов: {0: 1.0, 1: 1.2, 2: 2.0}\n",
      "epoch 0  | loss: 1.9861  | val_0_accuracy: 0.58712 |  0:00:01s\n",
      "epoch 1  | loss: 1.22394 | val_0_accuracy: 0.57955 |  0:00:02s\n",
      "epoch 2  | loss: 1.05896 | val_0_accuracy: 0.58712 |  0:00:03s\n",
      "epoch 3  | loss: 1.03916 | val_0_accuracy: 0.49621 |  0:00:04s\n",
      "epoch 4  | loss: 1.01917 | val_0_accuracy: 0.59091 |  0:00:04s\n",
      "epoch 5  | loss: 0.99369 | val_0_accuracy: 0.5     |  0:00:05s\n",
      "epoch 6  | loss: 0.96717 | val_0_accuracy: 0.625   |  0:00:06s\n",
      "epoch 7  | loss: 0.95142 | val_0_accuracy: 0.61742 |  0:00:07s\n",
      "epoch 8  | loss: 0.92837 | val_0_accuracy: 0.57955 |  0:00:08s\n",
      "epoch 9  | loss: 0.93484 | val_0_accuracy: 0.63258 |  0:00:08s\n",
      "epoch 10 | loss: 0.94141 | val_0_accuracy: 0.64773 |  0:00:09s\n",
      "epoch 11 | loss: 0.92336 | val_0_accuracy: 0.60606 |  0:00:10s\n",
      "epoch 12 | loss: 0.92785 | val_0_accuracy: 0.49242 |  0:00:11s\n",
      "epoch 13 | loss: 0.89574 | val_0_accuracy: 0.62879 |  0:00:12s\n",
      "epoch 14 | loss: 0.89501 | val_0_accuracy: 0.62121 |  0:00:12s\n",
      "epoch 15 | loss: 0.91094 | val_0_accuracy: 0.58712 |  0:00:13s\n",
      "epoch 16 | loss: 0.91829 | val_0_accuracy: 0.53788 |  0:00:14s\n",
      "epoch 17 | loss: 0.91552 | val_0_accuracy: 0.63636 |  0:00:15s\n",
      "epoch 18 | loss: 0.87566 | val_0_accuracy: 0.62121 |  0:00:16s\n",
      "epoch 19 | loss: 0.89981 | val_0_accuracy: 0.5947  |  0:00:16s\n",
      "epoch 20 | loss: 0.88472 | val_0_accuracy: 0.62879 |  0:00:17s\n",
      "epoch 21 | loss: 0.88952 | val_0_accuracy: 0.60606 |  0:00:18s\n",
      "epoch 22 | loss: 0.8723  | val_0_accuracy: 0.55682 |  0:00:19s\n",
      "epoch 23 | loss: 0.86466 | val_0_accuracy: 0.60985 |  0:00:19s\n",
      "epoch 24 | loss: 0.87308 | val_0_accuracy: 0.62879 |  0:00:20s\n",
      "epoch 25 | loss: 0.8778  | val_0_accuracy: 0.60227 |  0:00:21s\n",
      "epoch 26 | loss: 0.86292 | val_0_accuracy: 0.60227 |  0:00:22s\n",
      "epoch 27 | loss: 0.84795 | val_0_accuracy: 0.59848 |  0:00:23s\n",
      "epoch 28 | loss: 0.85364 | val_0_accuracy: 0.625   |  0:00:23s\n",
      "epoch 29 | loss: 0.85885 | val_0_accuracy: 0.60985 |  0:00:24s\n",
      "epoch 30 | loss: 0.86505 | val_0_accuracy: 0.625   |  0:00:25s\n",
      "epoch 31 | loss: 0.84815 | val_0_accuracy: 0.62879 |  0:00:26s\n",
      "epoch 32 | loss: 0.84322 | val_0_accuracy: 0.60606 |  0:00:27s\n",
      "epoch 33 | loss: 0.84259 | val_0_accuracy: 0.60227 |  0:00:27s\n",
      "epoch 34 | loss: 0.84799 | val_0_accuracy: 0.62121 |  0:00:28s\n",
      "epoch 35 | loss: 0.85892 | val_0_accuracy: 0.61364 |  0:00:29s\n",
      "epoch 36 | loss: 0.83359 | val_0_accuracy: 0.5947  |  0:00:30s\n",
      "epoch 37 | loss: 0.83959 | val_0_accuracy: 0.57197 |  0:00:31s\n",
      "epoch 38 | loss: 0.83046 | val_0_accuracy: 0.625   |  0:00:31s\n",
      "epoch 39 | loss: 0.82052 | val_0_accuracy: 0.60985 |  0:00:32s\n",
      "epoch 40 | loss: 0.83323 | val_0_accuracy: 0.64773 |  0:00:33s\n",
      "epoch 41 | loss: 0.84693 | val_0_accuracy: 0.60985 |  0:00:34s\n",
      "epoch 42 | loss: 0.81904 | val_0_accuracy: 0.60985 |  0:00:34s\n",
      "epoch 43 | loss: 0.8294  | val_0_accuracy: 0.64773 |  0:00:35s\n",
      "epoch 44 | loss: 0.83144 | val_0_accuracy: 0.60606 |  0:00:36s\n",
      "epoch 45 | loss: 0.82161 | val_0_accuracy: 0.62121 |  0:00:37s\n",
      "epoch 46 | loss: 0.81668 | val_0_accuracy: 0.64773 |  0:00:38s\n",
      "epoch 47 | loss: 0.81746 | val_0_accuracy: 0.60227 |  0:00:38s\n",
      "epoch 48 | loss: 0.81455 | val_0_accuracy: 0.59848 |  0:00:39s\n",
      "epoch 49 | loss: 0.83126 | val_0_accuracy: 0.625   |  0:00:40s\n",
      "epoch 50 | loss: 0.81863 | val_0_accuracy: 0.56061 |  0:00:41s\n",
      "epoch 51 | loss: 0.81477 | val_0_accuracy: 0.625   |  0:00:42s\n",
      "epoch 52 | loss: 0.81736 | val_0_accuracy: 0.61742 |  0:00:43s\n",
      "epoch 53 | loss: 0.80524 | val_0_accuracy: 0.64773 |  0:00:43s\n",
      "epoch 54 | loss: 0.80959 | val_0_accuracy: 0.65152 |  0:00:44s\n",
      "epoch 55 | loss: 0.80003 | val_0_accuracy: 0.65152 |  0:00:45s\n",
      "epoch 56 | loss: 0.82033 | val_0_accuracy: 0.64773 |  0:00:46s\n",
      "epoch 57 | loss: 0.81422 | val_0_accuracy: 0.64015 |  0:00:46s\n",
      "epoch 58 | loss: 0.81819 | val_0_accuracy: 0.56818 |  0:00:47s\n",
      "epoch 59 | loss: 0.81109 | val_0_accuracy: 0.62121 |  0:00:48s\n",
      "epoch 60 | loss: 0.81272 | val_0_accuracy: 0.64015 |  0:00:49s\n",
      "epoch 61 | loss: 0.80921 | val_0_accuracy: 0.60606 |  0:00:50s\n",
      "epoch 62 | loss: 0.81312 | val_0_accuracy: 0.60985 |  0:00:50s\n",
      "epoch 63 | loss: 0.81066 | val_0_accuracy: 0.61364 |  0:00:51s\n",
      "epoch 64 | loss: 0.81518 | val_0_accuracy: 0.61364 |  0:00:52s\n",
      "epoch 65 | loss: 0.80463 | val_0_accuracy: 0.60227 |  0:00:53s\n",
      "epoch 66 | loss: 0.79723 | val_0_accuracy: 0.58333 |  0:00:54s\n",
      "epoch 67 | loss: 0.8129  | val_0_accuracy: 0.63258 |  0:00:54s\n",
      "epoch 68 | loss: 0.81866 | val_0_accuracy: 0.59091 |  0:00:55s\n",
      "epoch 69 | loss: 0.80197 | val_0_accuracy: 0.61742 |  0:00:56s\n",
      "epoch 70 | loss: 0.8092  | val_0_accuracy: 0.61742 |  0:00:57s\n",
      "epoch 71 | loss: 0.80926 | val_0_accuracy: 0.61364 |  0:00:58s\n",
      "epoch 72 | loss: 0.8118  | val_0_accuracy: 0.625   |  0:00:58s\n",
      "epoch 73 | loss: 0.80098 | val_0_accuracy: 0.63636 |  0:00:59s\n",
      "epoch 74 | loss: 0.80196 | val_0_accuracy: 0.625   |  0:01:00s\n",
      "epoch 75 | loss: 0.79468 | val_0_accuracy: 0.63636 |  0:01:01s\n",
      "epoch 76 | loss: 0.79655 | val_0_accuracy: 0.62879 |  0:01:02s\n",
      "epoch 77 | loss: 0.79648 | val_0_accuracy: 0.62879 |  0:01:03s\n",
      "epoch 78 | loss: 0.79612 | val_0_accuracy: 0.63258 |  0:01:04s\n",
      "epoch 79 | loss: 0.79325 | val_0_accuracy: 0.61742 |  0:01:05s\n",
      "epoch 80 | loss: 0.80497 | val_0_accuracy: 0.62121 |  0:01:05s\n",
      "epoch 81 | loss: 0.80407 | val_0_accuracy: 0.61364 |  0:01:06s\n",
      "epoch 82 | loss: 0.78918 | val_0_accuracy: 0.625   |  0:01:07s\n",
      "epoch 83 | loss: 0.78701 | val_0_accuracy: 0.61364 |  0:01:08s\n",
      "epoch 84 | loss: 0.7892  | val_0_accuracy: 0.625   |  0:01:09s\n",
      "epoch 85 | loss: 0.79661 | val_0_accuracy: 0.61364 |  0:01:09s\n",
      "epoch 86 | loss: 0.79161 | val_0_accuracy: 0.63258 |  0:01:10s\n",
      "epoch 87 | loss: 0.79996 | val_0_accuracy: 0.63636 |  0:01:11s\n",
      "epoch 88 | loss: 0.79753 | val_0_accuracy: 0.61742 |  0:01:12s\n",
      "epoch 89 | loss: 0.79679 | val_0_accuracy: 0.59848 |  0:01:13s\n",
      "epoch 90 | loss: 0.803   | val_0_accuracy: 0.63258 |  0:01:13s\n",
      "epoch 91 | loss: 0.80313 | val_0_accuracy: 0.625   |  0:01:14s\n",
      "epoch 92 | loss: 0.8011  | val_0_accuracy: 0.60985 |  0:01:15s\n",
      "epoch 93 | loss: 0.79579 | val_0_accuracy: 0.62121 |  0:01:16s\n",
      "epoch 94 | loss: 0.80777 | val_0_accuracy: 0.62879 |  0:01:16s\n",
      "epoch 95 | loss: 0.80198 | val_0_accuracy: 0.64773 |  0:01:17s\n",
      "epoch 96 | loss: 0.80866 | val_0_accuracy: 0.625   |  0:01:18s\n",
      "epoch 97 | loss: 0.79146 | val_0_accuracy: 0.625   |  0:01:19s\n",
      "epoch 98 | loss: 0.80119 | val_0_accuracy: 0.62879 |  0:01:20s\n",
      "epoch 99 | loss: 0.7979  | val_0_accuracy: 0.61742 |  0:01:20s\n",
      "epoch 100| loss: 0.78211 | val_0_accuracy: 0.64394 |  0:01:21s\n",
      "epoch 101| loss: 0.80584 | val_0_accuracy: 0.64015 |  0:01:22s\n",
      "epoch 102| loss: 0.79134 | val_0_accuracy: 0.63258 |  0:01:23s\n",
      "epoch 103| loss: 0.78651 | val_0_accuracy: 0.63258 |  0:01:24s\n",
      "epoch 104| loss: 0.79459 | val_0_accuracy: 0.62879 |  0:01:25s\n",
      "epoch 105| loss: 0.78627 | val_0_accuracy: 0.64015 |  0:01:25s\n",
      "epoch 106| loss: 0.78731 | val_0_accuracy: 0.62879 |  0:01:26s\n",
      "epoch 107| loss: 0.79647 | val_0_accuracy: 0.63258 |  0:01:27s\n",
      "epoch 108| loss: 0.78636 | val_0_accuracy: 0.64394 |  0:01:28s\n",
      "epoch 109| loss: 0.7841  | val_0_accuracy: 0.64015 |  0:01:28s\n",
      "epoch 110| loss: 0.80251 | val_0_accuracy: 0.63636 |  0:01:29s\n",
      "epoch 111| loss: 0.78581 | val_0_accuracy: 0.61364 |  0:01:30s\n",
      "epoch 112| loss: 0.78109 | val_0_accuracy: 0.62879 |  0:01:31s\n",
      "epoch 113| loss: 0.78324 | val_0_accuracy: 0.64015 |  0:01:32s\n",
      "epoch 114| loss: 0.78214 | val_0_accuracy: 0.625   |  0:01:33s\n",
      "epoch 115| loss: 0.79301 | val_0_accuracy: 0.64015 |  0:01:34s\n",
      "epoch 116| loss: 0.79491 | val_0_accuracy: 0.63636 |  0:01:34s\n",
      "epoch 117| loss: 0.7848  | val_0_accuracy: 0.62879 |  0:01:35s\n",
      "epoch 118| loss: 0.79671 | val_0_accuracy: 0.6553  |  0:01:36s\n",
      "epoch 119| loss: 0.78727 | val_0_accuracy: 0.64394 |  0:01:37s\n",
      "epoch 120| loss: 0.77821 | val_0_accuracy: 0.63258 |  0:01:37s\n",
      "epoch 121| loss: 0.78809 | val_0_accuracy: 0.625   |  0:01:38s\n",
      "epoch 122| loss: 0.78697 | val_0_accuracy: 0.64015 |  0:01:39s\n",
      "epoch 123| loss: 0.78944 | val_0_accuracy: 0.62879 |  0:01:40s\n",
      "epoch 124| loss: 0.77946 | val_0_accuracy: 0.64394 |  0:01:40s\n",
      "epoch 125| loss: 0.77368 | val_0_accuracy: 0.625   |  0:01:41s\n",
      "epoch 126| loss: 0.78652 | val_0_accuracy: 0.625   |  0:01:42s\n",
      "epoch 127| loss: 0.79252 | val_0_accuracy: 0.64394 |  0:01:43s\n",
      "epoch 128| loss: 0.78441 | val_0_accuracy: 0.62121 |  0:01:44s\n",
      "epoch 129| loss: 0.78735 | val_0_accuracy: 0.62121 |  0:01:45s\n",
      "epoch 130| loss: 0.78991 | val_0_accuracy: 0.60985 |  0:01:45s\n",
      "epoch 131| loss: 0.77489 | val_0_accuracy: 0.63258 |  0:01:46s\n",
      "epoch 132| loss: 0.77329 | val_0_accuracy: 0.60985 |  0:01:47s\n",
      "epoch 133| loss: 0.78737 | val_0_accuracy: 0.63258 |  0:01:48s\n",
      "epoch 134| loss: 0.79385 | val_0_accuracy: 0.625   |  0:01:49s\n",
      "epoch 135| loss: 0.78519 | val_0_accuracy: 0.61364 |  0:01:49s\n",
      "epoch 136| loss: 0.78468 | val_0_accuracy: 0.65152 |  0:01:50s\n",
      "epoch 137| loss: 0.7719  | val_0_accuracy: 0.64394 |  0:01:51s\n",
      "epoch 138| loss: 0.78361 | val_0_accuracy: 0.64394 |  0:01:52s\n",
      "epoch 139| loss: 0.78618 | val_0_accuracy: 0.63258 |  0:01:53s\n",
      "epoch 140| loss: 0.78725 | val_0_accuracy: 0.63636 |  0:01:53s\n",
      "epoch 141| loss: 0.78899 | val_0_accuracy: 0.64015 |  0:01:54s\n",
      "epoch 142| loss: 0.78439 | val_0_accuracy: 0.6553  |  0:01:55s\n",
      "epoch 143| loss: 0.7764  | val_0_accuracy: 0.63636 |  0:01:56s\n",
      "epoch 144| loss: 0.77834 | val_0_accuracy: 0.63636 |  0:01:57s\n",
      "epoch 145| loss: 0.77368 | val_0_accuracy: 0.64015 |  0:01:57s\n",
      "epoch 146| loss: 0.7883  | val_0_accuracy: 0.64015 |  0:01:58s\n",
      "epoch 147| loss: 0.77938 | val_0_accuracy: 0.64394 |  0:01:59s\n",
      "epoch 148| loss: 0.76446 | val_0_accuracy: 0.63636 |  0:02:00s\n",
      "epoch 149| loss: 0.77934 | val_0_accuracy: 0.63258 |  0:02:00s\n",
      "epoch 150| loss: 0.77172 | val_0_accuracy: 0.63258 |  0:02:01s\n",
      "epoch 151| loss: 0.77289 | val_0_accuracy: 0.625   |  0:02:02s\n",
      "epoch 152| loss: 0.76527 | val_0_accuracy: 0.62879 |  0:02:03s\n",
      "epoch 153| loss: 0.76794 | val_0_accuracy: 0.63258 |  0:02:04s\n",
      "epoch 154| loss: 0.77837 | val_0_accuracy: 0.625   |  0:02:04s\n",
      "epoch 155| loss: 0.76514 | val_0_accuracy: 0.65152 |  0:02:05s\n",
      "epoch 156| loss: 0.7787  | val_0_accuracy: 0.625   |  0:02:06s\n",
      "epoch 157| loss: 0.77106 | val_0_accuracy: 0.62121 |  0:02:07s\n",
      "epoch 158| loss: 0.77351 | val_0_accuracy: 0.625   |  0:02:07s\n",
      "epoch 159| loss: 0.77421 | val_0_accuracy: 0.63258 |  0:02:08s\n",
      "epoch 160| loss: 0.77398 | val_0_accuracy: 0.63258 |  0:02:09s\n",
      "epoch 161| loss: 0.78375 | val_0_accuracy: 0.61742 |  0:02:10s\n",
      "epoch 162| loss: 0.77052 | val_0_accuracy: 0.62879 |  0:02:11s\n",
      "epoch 163| loss: 0.75864 | val_0_accuracy: 0.625   |  0:02:12s\n",
      "epoch 164| loss: 0.76817 | val_0_accuracy: 0.63258 |  0:02:12s\n",
      "epoch 165| loss: 0.76741 | val_0_accuracy: 0.62121 |  0:02:13s\n",
      "epoch 166| loss: 0.75828 | val_0_accuracy: 0.62121 |  0:02:14s\n",
      "epoch 167| loss: 0.76533 | val_0_accuracy: 0.59848 |  0:02:15s\n",
      "epoch 168| loss: 0.77901 | val_0_accuracy: 0.60985 |  0:02:16s\n",
      "epoch 169| loss: 0.7662  | val_0_accuracy: 0.63636 |  0:02:17s\n",
      "epoch 170| loss: 0.78175 | val_0_accuracy: 0.60227 |  0:02:18s\n",
      "epoch 171| loss: 0.77234 | val_0_accuracy: 0.61364 |  0:02:18s\n",
      "epoch 172| loss: 0.7622  | val_0_accuracy: 0.60985 |  0:02:19s\n",
      "epoch 173| loss: 0.77183 | val_0_accuracy: 0.60985 |  0:02:20s\n",
      "epoch 174| loss: 0.78201 | val_0_accuracy: 0.61364 |  0:02:21s\n",
      "epoch 175| loss: 0.77506 | val_0_accuracy: 0.63636 |  0:02:22s\n",
      "epoch 176| loss: 0.76744 | val_0_accuracy: 0.61742 |  0:02:23s\n",
      "epoch 177| loss: 0.77582 | val_0_accuracy: 0.62121 |  0:02:24s\n",
      "epoch 178| loss: 0.78009 | val_0_accuracy: 0.62121 |  0:02:25s\n",
      "epoch 179| loss: 0.76966 | val_0_accuracy: 0.625   |  0:02:25s\n",
      "epoch 180| loss: 0.76403 | val_0_accuracy: 0.62879 |  0:02:26s\n",
      "epoch 181| loss: 0.75915 | val_0_accuracy: 0.62879 |  0:02:27s\n",
      "epoch 182| loss: 0.77371 | val_0_accuracy: 0.61742 |  0:02:28s\n",
      "epoch 183| loss: 0.76402 | val_0_accuracy: 0.62121 |  0:02:29s\n",
      "epoch 184| loss: 0.77197 | val_0_accuracy: 0.60985 |  0:02:30s\n",
      "epoch 185| loss: 0.75999 | val_0_accuracy: 0.62121 |  0:02:31s\n",
      "epoch 186| loss: 0.75371 | val_0_accuracy: 0.61742 |  0:02:32s\n",
      "epoch 187| loss: 0.76004 | val_0_accuracy: 0.63636 |  0:02:33s\n",
      "epoch 188| loss: 0.76711 | val_0_accuracy: 0.66667 |  0:02:34s\n",
      "epoch 189| loss: 0.76187 | val_0_accuracy: 0.6553  |  0:02:35s\n",
      "epoch 190| loss: 0.76694 | val_0_accuracy: 0.63636 |  0:02:36s\n",
      "epoch 191| loss: 0.75169 | val_0_accuracy: 0.64015 |  0:02:37s\n",
      "epoch 192| loss: 0.76469 | val_0_accuracy: 0.63258 |  0:02:37s\n",
      "epoch 193| loss: 0.77006 | val_0_accuracy: 0.63636 |  0:02:38s\n",
      "epoch 194| loss: 0.76534 | val_0_accuracy: 0.63258 |  0:02:39s\n",
      "epoch 195| loss: 0.76906 | val_0_accuracy: 0.63636 |  0:02:40s\n",
      "epoch 196| loss: 0.76014 | val_0_accuracy: 0.64394 |  0:02:41s\n",
      "epoch 197| loss: 0.76337 | val_0_accuracy: 0.63636 |  0:02:41s\n",
      "epoch 198| loss: 0.76753 | val_0_accuracy: 0.62879 |  0:02:42s\n",
      "epoch 199| loss: 0.75751 | val_0_accuracy: 0.625   |  0:02:43s\n",
      "epoch 200| loss: 0.76305 | val_0_accuracy: 0.625   |  0:02:44s\n",
      "epoch 201| loss: 0.77064 | val_0_accuracy: 0.62879 |  0:02:44s\n",
      "epoch 202| loss: 0.76472 | val_0_accuracy: 0.61364 |  0:02:45s\n",
      "epoch 203| loss: 0.75749 | val_0_accuracy: 0.6553  |  0:02:46s\n",
      "epoch 204| loss: 0.76474 | val_0_accuracy: 0.63258 |  0:02:47s\n",
      "epoch 205| loss: 0.75685 | val_0_accuracy: 0.60606 |  0:02:47s\n",
      "epoch 206| loss: 0.76034 | val_0_accuracy: 0.63258 |  0:02:48s\n",
      "epoch 207| loss: 0.76028 | val_0_accuracy: 0.57955 |  0:02:49s\n",
      "epoch 208| loss: 0.76289 | val_0_accuracy: 0.64015 |  0:02:50s\n",
      "epoch 209| loss: 0.76224 | val_0_accuracy: 0.63258 |  0:02:50s\n",
      "epoch 210| loss: 0.75571 | val_0_accuracy: 0.61364 |  0:02:51s\n",
      "epoch 211| loss: 0.746   | val_0_accuracy: 0.62121 |  0:02:52s\n",
      "epoch 212| loss: 0.76814 | val_0_accuracy: 0.61742 |  0:02:53s\n",
      "epoch 213| loss: 0.76113 | val_0_accuracy: 0.6553  |  0:02:54s\n",
      "epoch 214| loss: 0.76283 | val_0_accuracy: 0.61742 |  0:02:55s\n",
      "epoch 215| loss: 0.76576 | val_0_accuracy: 0.625   |  0:02:55s\n",
      "epoch 216| loss: 0.76187 | val_0_accuracy: 0.63258 |  0:02:56s\n",
      "epoch 217| loss: 0.76074 | val_0_accuracy: 0.59848 |  0:02:57s\n",
      "epoch 218| loss: 0.7664  | val_0_accuracy: 0.61364 |  0:02:57s\n",
      "epoch 219| loss: 0.75474 | val_0_accuracy: 0.60606 |  0:02:58s\n",
      "epoch 220| loss: 0.75957 | val_0_accuracy: 0.62121 |  0:02:59s\n",
      "epoch 221| loss: 0.7594  | val_0_accuracy: 0.60227 |  0:03:00s\n",
      "epoch 222| loss: 0.75965 | val_0_accuracy: 0.60606 |  0:03:01s\n",
      "epoch 223| loss: 0.75872 | val_0_accuracy: 0.60985 |  0:03:01s\n",
      "epoch 224| loss: 0.76471 | val_0_accuracy: 0.60606 |  0:03:02s\n",
      "epoch 225| loss: 0.75757 | val_0_accuracy: 0.61742 |  0:03:03s\n",
      "epoch 226| loss: 0.75693 | val_0_accuracy: 0.60985 |  0:03:04s\n",
      "epoch 227| loss: 0.74213 | val_0_accuracy: 0.625   |  0:03:04s\n",
      "epoch 228| loss: 0.74233 | val_0_accuracy: 0.625   |  0:03:05s\n",
      "epoch 229| loss: 0.75756 | val_0_accuracy: 0.60985 |  0:03:06s\n",
      "epoch 230| loss: 0.76592 | val_0_accuracy: 0.61742 |  0:03:07s\n",
      "epoch 231| loss: 0.75946 | val_0_accuracy: 0.60227 |  0:03:07s\n",
      "epoch 232| loss: 0.7607  | val_0_accuracy: 0.60227 |  0:03:08s\n",
      "epoch 233| loss: 0.74756 | val_0_accuracy: 0.60606 |  0:03:09s\n",
      "epoch 234| loss: 0.76582 | val_0_accuracy: 0.61364 |  0:03:10s\n",
      "epoch 235| loss: 0.75805 | val_0_accuracy: 0.61364 |  0:03:11s\n",
      "epoch 236| loss: 0.75236 | val_0_accuracy: 0.61742 |  0:03:12s\n",
      "epoch 237| loss: 0.75732 | val_0_accuracy: 0.61364 |  0:03:13s\n",
      "epoch 238| loss: 0.75053 | val_0_accuracy: 0.60606 |  0:03:13s\n",
      "epoch 239| loss: 0.76798 | val_0_accuracy: 0.61742 |  0:03:14s\n",
      "epoch 240| loss: 0.76587 | val_0_accuracy: 0.60985 |  0:03:15s\n",
      "epoch 241| loss: 0.75777 | val_0_accuracy: 0.59848 |  0:03:16s\n",
      "epoch 242| loss: 0.74931 | val_0_accuracy: 0.62879 |  0:03:17s\n",
      "epoch 243| loss: 0.76071 | val_0_accuracy: 0.56818 |  0:03:18s\n",
      "epoch 244| loss: 0.74912 | val_0_accuracy: 0.5947  |  0:03:19s\n",
      "epoch 245| loss: 0.77018 | val_0_accuracy: 0.58712 |  0:03:19s\n",
      "epoch 246| loss: 0.76136 | val_0_accuracy: 0.58712 |  0:03:20s\n",
      "epoch 247| loss: 0.75656 | val_0_accuracy: 0.60985 |  0:03:21s\n",
      "epoch 248| loss: 0.7572  | val_0_accuracy: 0.59091 |  0:03:22s\n",
      "epoch 249| loss: 0.74854 | val_0_accuracy: 0.59091 |  0:03:23s\n",
      "epoch 250| loss: 0.77096 | val_0_accuracy: 0.60606 |  0:03:23s\n",
      "epoch 251| loss: 0.75667 | val_0_accuracy: 0.61364 |  0:03:24s\n",
      "epoch 252| loss: 0.75481 | val_0_accuracy: 0.59091 |  0:03:25s\n",
      "epoch 253| loss: 0.74669 | val_0_accuracy: 0.60227 |  0:03:26s\n",
      "epoch 254| loss: 0.73515 | val_0_accuracy: 0.58333 |  0:03:26s\n",
      "epoch 255| loss: 0.75894 | val_0_accuracy: 0.58333 |  0:03:27s\n",
      "epoch 256| loss: 0.75137 | val_0_accuracy: 0.57576 |  0:03:28s\n",
      "epoch 257| loss: 0.75934 | val_0_accuracy: 0.58712 |  0:03:29s\n",
      "epoch 258| loss: 0.75529 | val_0_accuracy: 0.60985 |  0:03:29s\n",
      "epoch 259| loss: 0.75431 | val_0_accuracy: 0.59848 |  0:03:30s\n",
      "epoch 260| loss: 0.75129 | val_0_accuracy: 0.58712 |  0:03:31s\n",
      "epoch 261| loss: 0.7537  | val_0_accuracy: 0.58712 |  0:03:32s\n",
      "epoch 262| loss: 0.76233 | val_0_accuracy: 0.60227 |  0:03:33s\n",
      "epoch 263| loss: 0.74953 | val_0_accuracy: 0.61364 |  0:03:33s\n",
      "epoch 264| loss: 0.76351 | val_0_accuracy: 0.58712 |  0:03:34s\n",
      "epoch 265| loss: 0.74846 | val_0_accuracy: 0.58712 |  0:03:35s\n",
      "epoch 266| loss: 0.76722 | val_0_accuracy: 0.59848 |  0:03:36s\n",
      "epoch 267| loss: 0.76353 | val_0_accuracy: 0.61364 |  0:03:37s\n",
      "epoch 268| loss: 0.75539 | val_0_accuracy: 0.60227 |  0:03:37s\n",
      "epoch 269| loss: 0.75553 | val_0_accuracy: 0.61364 |  0:03:38s\n",
      "epoch 270| loss: 0.75199 | val_0_accuracy: 0.57955 |  0:03:39s\n",
      "epoch 271| loss: 0.755   | val_0_accuracy: 0.63636 |  0:03:40s\n",
      "epoch 272| loss: 0.74446 | val_0_accuracy: 0.63258 |  0:03:41s\n",
      "epoch 273| loss: 0.75668 | val_0_accuracy: 0.60606 |  0:03:41s\n",
      "epoch 274| loss: 0.75253 | val_0_accuracy: 0.60227 |  0:03:42s\n",
      "epoch 275| loss: 0.74837 | val_0_accuracy: 0.59848 |  0:03:43s\n",
      "epoch 276| loss: 0.75134 | val_0_accuracy: 0.62121 |  0:03:44s\n",
      "epoch 277| loss: 0.75355 | val_0_accuracy: 0.625   |  0:03:45s\n",
      "epoch 278| loss: 0.75235 | val_0_accuracy: 0.62879 |  0:03:45s\n",
      "epoch 279| loss: 0.73802 | val_0_accuracy: 0.62879 |  0:03:46s\n",
      "epoch 280| loss: 0.75763 | val_0_accuracy: 0.61742 |  0:03:47s\n",
      "epoch 281| loss: 0.75272 | val_0_accuracy: 0.60606 |  0:03:48s\n",
      "epoch 282| loss: 0.75175 | val_0_accuracy: 0.62879 |  0:03:49s\n",
      "epoch 283| loss: 0.73989 | val_0_accuracy: 0.625   |  0:03:49s\n",
      "epoch 284| loss: 0.75278 | val_0_accuracy: 0.60985 |  0:03:50s\n",
      "epoch 285| loss: 0.74411 | val_0_accuracy: 0.63258 |  0:03:51s\n",
      "epoch 286| loss: 0.73524 | val_0_accuracy: 0.61364 |  0:03:52s\n",
      "epoch 287| loss: 0.73692 | val_0_accuracy: 0.625   |  0:03:52s\n",
      "epoch 288| loss: 0.74331 | val_0_accuracy: 0.59848 |  0:03:53s\n",
      "\n",
      "Early stopping occurred at epoch 288 with best_epoch = 188 and best_val_0_accuracy = 0.66667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [376  32   1]\n",
      "Распределение истинных классов: [245 125  39]\n",
      "Accuracy: 0.5795\n",
      "F1-score: 0.4788\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.60      0.92      0.73       245\n",
      "          c1       0.34      0.09      0.14       125\n",
      "          c2       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.58       409\n",
      "   macro avg       0.31      0.34      0.29       409\n",
      "weighted avg       0.47      0.58      0.48       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " atr            0.192343\n",
      "high_low       0.123292\n",
      "low            0.092999\n",
      "high           0.088348\n",
      "open_diff_2    0.071955\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной с новым порогом\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.005\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление ATR\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "# key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "#                 'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "#                 'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr']\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.005, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train:\", np.bincount(y_train))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.0, 1: 1.2, 2: 2.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=128, n_a=128, n_steps=5, gamma=1.5,\n",
    "    lambda_sparse=0.005, optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=100,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=256,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfdd98c2-1682-4682-84dc-c6bbed4c996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_143/1219255241.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение y_train (после SMOTE): [5519 5519 5519]\n",
      "Распределение y_val: [149  92  23]\n",
      "Распределение y_test: [219 126  64]\n",
      "Веса классов: {0: 1.0, 1: 1.5, 2: 5.0}\n",
      "epoch 0  | loss: 1.34181 | val_0_accuracy: 0.14773 | val_0_balanced_accuracy: 0.37606 |  0:00:01s\n",
      "epoch 1  | loss: 0.96968 | val_0_accuracy: 0.24621 | val_0_balanced_accuracy: 0.35814 |  0:00:03s\n",
      "epoch 2  | loss: 0.94348 | val_0_accuracy: 0.28409 | val_0_balanced_accuracy: 0.40662 |  0:00:04s\n",
      "epoch 3  | loss: 0.87253 | val_0_accuracy: 0.26515 | val_0_balanced_accuracy: 0.33854 |  0:00:05s\n",
      "epoch 4  | loss: 0.83187 | val_0_accuracy: 0.28409 | val_0_balanced_accuracy: 0.3557  |  0:00:07s\n",
      "epoch 5  | loss: 0.79475 | val_0_accuracy: 0.31061 | val_0_balanced_accuracy: 0.35612 |  0:00:08s\n",
      "epoch 6  | loss: 0.77439 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.43624 |  0:00:10s\n",
      "epoch 7  | loss: 0.73645 | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.42058 |  0:00:11s\n",
      "epoch 8  | loss: 0.70939 | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.37336 |  0:00:13s\n",
      "epoch 9  | loss: 0.68807 | val_0_accuracy: 0.29167 | val_0_balanced_accuracy: 0.37616 |  0:00:14s\n",
      "epoch 10 | loss: 0.67861 | val_0_accuracy: 0.25379 | val_0_balanced_accuracy: 0.3427  |  0:00:16s\n",
      "epoch 11 | loss: 0.66836 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.39648 |  0:00:17s\n",
      "epoch 12 | loss: 0.65483 | val_0_accuracy: 0.33712 | val_0_balanced_accuracy: 0.39768 |  0:00:18s\n",
      "epoch 13 | loss: 0.65416 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.41163 |  0:00:20s\n",
      "epoch 14 | loss: 0.65331 | val_0_accuracy: 0.41667 | val_0_balanced_accuracy: 0.41876 |  0:00:21s\n",
      "epoch 15 | loss: 0.64961 | val_0_accuracy: 0.36742 | val_0_balanced_accuracy: 0.395   |  0:00:23s\n",
      "epoch 16 | loss: 0.62427 | val_0_accuracy: 0.40152 | val_0_balanced_accuracy: 0.42345 |  0:00:24s\n",
      "epoch 17 | loss: 0.64612 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.34654 |  0:00:26s\n",
      "epoch 18 | loss: 0.63053 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.41441 |  0:00:27s\n",
      "epoch 19 | loss: 0.64116 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.40609 |  0:00:29s\n",
      "epoch 20 | loss: 0.63915 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.41579 |  0:00:30s\n",
      "epoch 21 | loss: 0.63117 | val_0_accuracy: 0.35606 | val_0_balanced_accuracy: 0.40076 |  0:00:31s\n",
      "epoch 22 | loss: 0.64291 | val_0_accuracy: 0.30682 | val_0_balanced_accuracy: 0.35687 |  0:00:33s\n",
      "epoch 23 | loss: 0.63359 | val_0_accuracy: 0.32576 | val_0_balanced_accuracy: 0.41015 |  0:00:34s\n",
      "epoch 24 | loss: 0.62211 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.3932  |  0:00:36s\n",
      "epoch 25 | loss: 0.62806 | val_0_accuracy: 0.31061 | val_0_balanced_accuracy: 0.39011 |  0:00:37s\n",
      "epoch 26 | loss: 0.64813 | val_0_accuracy: 0.31061 | val_0_balanced_accuracy: 0.35685 |  0:00:39s\n",
      "epoch 27 | loss: 0.63299 | val_0_accuracy: 0.32197 | val_0_balanced_accuracy: 0.34087 |  0:00:40s\n",
      "epoch 28 | loss: 0.64232 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.37912 |  0:00:42s\n",
      "epoch 29 | loss: 0.61437 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.42121 |  0:00:43s\n",
      "epoch 30 | loss: 0.61353 | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.40577 |  0:00:44s\n",
      "epoch 31 | loss: 0.61485 | val_0_accuracy: 0.32576 | val_0_balanced_accuracy: 0.39053 |  0:00:46s\n",
      "epoch 32 | loss: 0.60877 | val_0_accuracy: 0.35606 | val_0_balanced_accuracy: 0.42527 |  0:00:47s\n",
      "epoch 33 | loss: 0.61492 | val_0_accuracy: 0.36364 | val_0_balanced_accuracy: 0.43274 |  0:00:49s\n",
      "epoch 34 | loss: 0.61248 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.37861 |  0:00:50s\n",
      "epoch 35 | loss: 0.60682 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.32553 |  0:00:51s\n",
      "epoch 36 | loss: 0.60545 | val_0_accuracy: 0.2803  | val_0_balanced_accuracy: 0.32458 |  0:00:53s\n",
      "epoch 37 | loss: 0.60355 | val_0_accuracy: 0.35606 | val_0_balanced_accuracy: 0.38413 |  0:00:54s\n",
      "epoch 38 | loss: 0.59482 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.37924 |  0:00:56s\n",
      "epoch 39 | loss: 0.60287 | val_0_accuracy: 0.29924 | val_0_balanced_accuracy: 0.36166 |  0:00:57s\n",
      "epoch 40 | loss: 0.60136 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.34333 |  0:00:58s\n",
      "epoch 41 | loss: 0.59161 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.39405 |  0:01:00s\n",
      "epoch 42 | loss: 0.59403 | val_0_accuracy: 0.29545 | val_0_balanced_accuracy: 0.34024 |  0:01:01s\n",
      "epoch 43 | loss: 0.60217 | val_0_accuracy: 0.28788 | val_0_balanced_accuracy: 0.36422 |  0:01:03s\n",
      "epoch 44 | loss: 0.59359 | val_0_accuracy: 0.29167 | val_0_balanced_accuracy: 0.33895 |  0:01:04s\n",
      "epoch 45 | loss: 0.58612 | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.39181 |  0:01:05s\n",
      "epoch 46 | loss: 0.61414 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.41302 |  0:01:07s\n",
      "epoch 47 | loss: 0.60744 | val_0_accuracy: 0.36364 | val_0_balanced_accuracy: 0.40064 |  0:01:08s\n",
      "epoch 48 | loss: 0.6102  | val_0_accuracy: 0.37879 | val_0_balanced_accuracy: 0.41652 |  0:01:09s\n",
      "epoch 49 | loss: 0.59153 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.43284 |  0:01:11s\n",
      "epoch 50 | loss: 0.59379 | val_0_accuracy: 0.36364 | val_0_balanced_accuracy: 0.42165 |  0:01:13s\n",
      "epoch 51 | loss: 0.60357 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.44475 |  0:01:14s\n",
      "epoch 52 | loss: 0.59887 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.40152 |  0:01:15s\n",
      "epoch 53 | loss: 0.5955  | val_0_accuracy: 0.32197 | val_0_balanced_accuracy: 0.42155 |  0:01:17s\n",
      "epoch 54 | loss: 0.59628 | val_0_accuracy: 0.30682 | val_0_balanced_accuracy: 0.39619 |  0:01:18s\n",
      "epoch 55 | loss: 0.60496 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.40247 |  0:01:20s\n",
      "epoch 56 | loss: 0.59349 | val_0_accuracy: 0.31439 | val_0_balanced_accuracy: 0.3898  |  0:01:21s\n",
      "epoch 57 | loss: 0.59448 | val_0_accuracy: 0.32576 | val_0_balanced_accuracy: 0.41154 |  0:01:22s\n",
      "epoch 58 | loss: 0.58478 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.40684 |  0:01:24s\n",
      "epoch 59 | loss: 0.589   | val_0_accuracy: 0.32197 | val_0_balanced_accuracy: 0.4174  |  0:01:25s\n",
      "epoch 60 | loss: 0.58297 | val_0_accuracy: 0.27652 | val_0_balanced_accuracy: 0.3834  |  0:01:27s\n",
      "epoch 61 | loss: 0.5905  | val_0_accuracy: 0.34848 | val_0_balanced_accuracy: 0.43999 |  0:01:28s\n",
      "epoch 62 | loss: 0.59999 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.41686 |  0:01:29s\n",
      "epoch 63 | loss: 0.59288 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.41217 |  0:01:31s\n",
      "epoch 64 | loss: 0.58703 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.41932 |  0:01:32s\n",
      "epoch 65 | loss: 0.59422 | val_0_accuracy: 0.37879 | val_0_balanced_accuracy: 0.43337 |  0:01:34s\n",
      "epoch 66 | loss: 0.58398 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.42005 |  0:01:35s\n",
      "epoch 67 | loss: 0.59005 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.40823 |  0:01:37s\n",
      "epoch 68 | loss: 0.59436 | val_0_accuracy: 0.32197 | val_0_balanced_accuracy: 0.38202 |  0:01:38s\n",
      "epoch 69 | loss: 0.59086 | val_0_accuracy: 0.31439 | val_0_balanced_accuracy: 0.41015 |  0:01:39s\n",
      "epoch 70 | loss: 0.58191 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.42549 |  0:01:41s\n",
      "epoch 71 | loss: 0.57187 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.39704 |  0:01:42s\n",
      "epoch 72 | loss: 0.57363 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.42953 |  0:01:43s\n",
      "epoch 73 | loss: 0.57862 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.43114 |  0:01:45s\n",
      "epoch 74 | loss: 0.57929 | val_0_accuracy: 0.29924 | val_0_balanced_accuracy: 0.37786 |  0:01:46s\n",
      "epoch 75 | loss: 0.56152 | val_0_accuracy: 0.34848 | val_0_balanced_accuracy: 0.44553 |  0:01:48s\n",
      "epoch 76 | loss: 0.5847  | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.40662 |  0:01:49s\n",
      "epoch 77 | loss: 0.5715  | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.39118 |  0:01:51s\n",
      "epoch 78 | loss: 0.57168 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.41567 |  0:01:52s\n",
      "epoch 79 | loss: 0.56789 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.4467  |  0:01:54s\n",
      "epoch 80 | loss: 0.56465 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.42187 |  0:01:55s\n",
      "epoch 81 | loss: 0.56447 | val_0_accuracy: 0.36364 | val_0_balanced_accuracy: 0.41771 |  0:01:57s\n",
      "epoch 82 | loss: 0.5508  | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.42868 |  0:01:58s\n",
      "epoch 83 | loss: 0.56515 | val_0_accuracy: 0.29924 | val_0_balanced_accuracy: 0.37487 |  0:01:59s\n",
      "epoch 84 | loss: 0.55604 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.40908 |  0:02:01s\n",
      "epoch 85 | loss: 0.56273 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.41355 |  0:02:02s\n",
      "epoch 86 | loss: 0.55181 | val_0_accuracy: 0.32197 | val_0_balanced_accuracy: 0.39661 |  0:02:04s\n",
      "epoch 87 | loss: 0.55894 | val_0_accuracy: 0.34848 | val_0_balanced_accuracy: 0.4386  |  0:02:05s\n",
      "epoch 88 | loss: 0.55552 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.38882 |  0:02:07s\n",
      "epoch 89 | loss: 0.55844 | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.37657 |  0:02:08s\n",
      "epoch 90 | loss: 0.55819 | val_0_accuracy: 0.32576 | val_0_balanced_accuracy: 0.39629 |  0:02:09s\n",
      "epoch 91 | loss: 0.54299 | val_0_accuracy: 0.31439 | val_0_balanced_accuracy: 0.38243 |  0:02:11s\n",
      "epoch 92 | loss: 0.57105 | val_0_accuracy: 0.32576 | val_0_balanced_accuracy: 0.403   |  0:02:12s\n",
      "epoch 93 | loss: 0.56544 | val_0_accuracy: 0.25379 | val_0_balanced_accuracy: 0.3605  |  0:02:13s\n",
      "epoch 94 | loss: 0.54884 | val_0_accuracy: 0.37879 | val_0_balanced_accuracy: 0.40842 |  0:02:15s\n",
      "epoch 95 | loss: 0.55713 | val_0_accuracy: 0.33712 | val_0_balanced_accuracy: 0.39768 |  0:02:16s\n",
      "epoch 96 | loss: 0.54954 | val_0_accuracy: 0.33333 | val_0_balanced_accuracy: 0.39821 |  0:02:17s\n",
      "epoch 97 | loss: 0.56133 | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.38381 |  0:02:19s\n",
      "epoch 98 | loss: 0.57216 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.42194 |  0:02:20s\n",
      "epoch 99 | loss: 0.56432 | val_0_accuracy: 0.30682 | val_0_balanced_accuracy: 0.36475 |  0:02:21s\n",
      "epoch 100| loss: 0.56386 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.36409 |  0:02:23s\n",
      "epoch 101| loss: 0.5648  | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.40108 |  0:02:24s\n",
      "epoch 102| loss: 0.5739  | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.39415 |  0:02:25s\n",
      "epoch 103| loss: 0.57063 | val_0_accuracy: 0.39394 | val_0_balanced_accuracy: 0.3887  |  0:02:27s\n",
      "epoch 104| loss: 0.5778  | val_0_accuracy: 0.34848 | val_0_balanced_accuracy: 0.43422 |  0:02:28s\n",
      "epoch 105| loss: 0.56834 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.44317 |  0:02:30s\n",
      "epoch 106| loss: 0.57514 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.38488 |  0:02:31s\n",
      "epoch 107| loss: 0.56172 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.38403 |  0:02:32s\n",
      "epoch 108| loss: 0.55719 | val_0_accuracy: 0.31439 | val_0_balanced_accuracy: 0.38936 |  0:02:34s\n",
      "epoch 109| loss: 0.56429 | val_0_accuracy: 0.29167 | val_0_balanced_accuracy: 0.3771  |  0:02:35s\n",
      "epoch 110| loss: 0.55154 | val_0_accuracy: 0.2803  | val_0_balanced_accuracy: 0.39928 |  0:02:36s\n",
      "epoch 111| loss: 0.5648  | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.37657 |  0:02:38s\n",
      "epoch 112| loss: 0.55669 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.39075 |  0:02:39s\n",
      "epoch 113| loss: 0.56398 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.39714 |  0:02:40s\n",
      "epoch 114| loss: 0.57143 | val_0_accuracy: 0.3447  | val_0_balanced_accuracy: 0.40609 |  0:02:42s\n",
      "epoch 115| loss: 0.5519  | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.40247 |  0:02:43s\n",
      "epoch 116| loss: 0.5506  | val_0_accuracy: 0.33712 | val_0_balanced_accuracy: 0.41781 |  0:02:45s\n",
      "epoch 117| loss: 0.55752 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.40385 |  0:02:46s\n",
      "epoch 118| loss: 0.55324 | val_0_accuracy: 0.36742 | val_0_balanced_accuracy: 0.429   |  0:02:47s\n",
      "epoch 119| loss: 0.54795 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.42304 |  0:02:48s\n",
      "epoch 120| loss: 0.55325 | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.42581 |  0:02:50s\n",
      "epoch 121| loss: 0.55963 | val_0_accuracy: 0.31818 | val_0_balanced_accuracy: 0.39575 |  0:02:51s\n",
      "epoch 122| loss: 0.56387 | val_0_accuracy: 0.35227 | val_0_balanced_accuracy: 0.40801 |  0:02:53s\n",
      "epoch 123| loss: 0.55684 | val_0_accuracy: 0.29545 | val_0_balanced_accuracy: 0.3851  |  0:02:54s\n",
      "epoch 124| loss: 0.56515 | val_0_accuracy: 0.33712 | val_0_balanced_accuracy: 0.42357 |  0:02:55s\n",
      "epoch 125| loss: 0.56442 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.41132 |  0:02:57s\n",
      "epoch 126| loss: 0.56942 | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.41737 |  0:02:58s\n",
      "epoch 127| loss: 0.5567  | val_0_accuracy: 0.28409 | val_0_balanced_accuracy: 0.35921 |  0:03:00s\n",
      "epoch 128| loss: 0.55064 | val_0_accuracy: 0.30682 | val_0_balanced_accuracy: 0.40312 |  0:03:01s\n",
      "epoch 129| loss: 0.55711 | val_0_accuracy: 0.37121 | val_0_balanced_accuracy: 0.42219 |  0:03:03s\n",
      "\n",
      "Early stopping occurred at epoch 129 with best_epoch = 79 and best_val_0_balanced_accuracy = 0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [ 72 198 139]\n",
      "Распределение истинных классов: [219 126  64]\n",
      "Accuracy: 0.3521\n",
      "F1-score: 0.3391\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.62      0.21      0.31       219\n",
      "          c1       0.29      0.46      0.36       126\n",
      "          c2       0.29      0.64      0.40        64\n",
      "\n",
      "    accuracy                           0.35       409\n",
      "   macro avg       0.40      0.44      0.36       409\n",
      "weighted avg       0.47      0.35      0.34       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " high_low    0.185959\n",
      "atr         0.145906\n",
      "close       0.128660\n",
      "volume      0.067386\n",
      "open        0.061043\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.006\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление признаков\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "bb = ta.bbands(df['close'], length=20)\n",
    "df['bb_width'] = (bb['BBU_20_2.0'] - bb['BBL_20_2.0']) / df['close']\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr', \n",
    "                'rsi', 'bb_width', 'volume_change']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf и NaN\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.005, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling с SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train (после SMOTE):\", np.bincount(y_train_res))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.0, 1: 1.5, 2: 5.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=128, n_a=128, n_steps=5, gamma=1.5,\n",
    "    lambda_sparse=0.001, optimizer_params=dict(lr=2e-3, weight_decay=1e-5),\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=256,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "740067cc-140a-4e52-9fa3-8b685f349561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные столбцы MACD: Index(['MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9'], dtype='object')\n",
      "Распределение y_train (после SMOTE): [5519 4000 3000]\n",
      "Распределение y_val: [149  92  23]\n",
      "Распределение y_test: [219 126  64]\n",
      "Веса классов: {0: 1.0, 1: 1.2, 2: 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.02174 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.32611 |  0:00:01s\n",
      "epoch 1  | loss: 1.57127 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.31196 |  0:00:03s\n",
      "epoch 2  | loss: 1.30817 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.34501 |  0:00:04s\n",
      "epoch 3  | loss: 1.18249 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.38899 |  0:00:05s\n",
      "epoch 4  | loss: 1.17537 | val_0_accuracy: 0.34091 | val_0_balanced_accuracy: 0.38861 |  0:00:07s\n",
      "epoch 5  | loss: 1.15593 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.3302  |  0:00:08s\n",
      "epoch 6  | loss: 1.12661 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.37484 |  0:00:10s\n",
      "epoch 7  | loss: 1.05303 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.39157 |  0:00:11s\n",
      "epoch 8  | loss: 1.05431 | val_0_accuracy: 0.41667 | val_0_balanced_accuracy: 0.32232 |  0:00:13s\n",
      "epoch 9  | loss: 1.06053 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.34598 |  0:00:14s\n",
      "epoch 10 | loss: 1.03987 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.31313 |  0:00:15s\n",
      "epoch 11 | loss: 1.03447 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.37326 |  0:00:17s\n",
      "epoch 12 | loss: 1.06785 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.36246 |  0:00:18s\n",
      "epoch 13 | loss: 1.02398 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.37387 |  0:00:20s\n",
      "epoch 14 | loss: 0.99473 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.36492 |  0:00:21s\n",
      "epoch 15 | loss: 0.99881 | val_0_accuracy: 0.36742 | val_0_balanced_accuracy: 0.35823 |  0:00:22s\n",
      "epoch 16 | loss: 1.00321 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.38955 |  0:00:24s\n",
      "epoch 17 | loss: 0.97185 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.32069 |  0:00:25s\n",
      "epoch 18 | loss: 0.97937 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.34488 |  0:00:27s\n",
      "epoch 19 | loss: 0.97638 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.35203 |  0:00:28s\n",
      "epoch 20 | loss: 0.94402 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.33635 |  0:00:30s\n",
      "epoch 21 | loss: 0.92874 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.35099 |  0:00:31s\n",
      "epoch 22 | loss: 0.94759 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.40947 |  0:00:32s\n",
      "epoch 23 | loss: 0.91616 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.39977 |  0:00:34s\n",
      "epoch 24 | loss: 0.96946 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.40711 |  0:00:35s\n",
      "epoch 25 | loss: 0.9285  | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.42953 |  0:00:37s\n",
      "epoch 26 | loss: 0.93885 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.34549 |  0:00:38s\n",
      "epoch 27 | loss: 0.93507 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.37312 |  0:00:39s\n",
      "epoch 28 | loss: 0.94464 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.33742 |  0:00:41s\n",
      "epoch 29 | loss: 0.92139 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.34413 |  0:00:42s\n",
      "epoch 30 | loss: 0.90418 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.3469  |  0:00:43s\n",
      "epoch 31 | loss: 0.89911 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.33978 |  0:00:45s\n",
      "epoch 32 | loss: 0.91642 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.3487  |  0:00:46s\n",
      "epoch 33 | loss: 0.89071 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.36395 |  0:00:47s\n",
      "epoch 34 | loss: 0.89242 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.33134 |  0:00:49s\n",
      "epoch 35 | loss: 0.88889 | val_0_accuracy: 0.5303  | val_0_balanced_accuracy: 0.34763 |  0:00:50s\n",
      "epoch 36 | loss: 0.88168 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.32207 |  0:00:52s\n",
      "epoch 37 | loss: 0.88659 | val_0_accuracy: 0.54924 | val_0_balanced_accuracy: 0.39347 |  0:00:53s\n",
      "epoch 38 | loss: 0.90473 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.35446 |  0:00:54s\n",
      "epoch 39 | loss: 0.89517 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.33902 |  0:00:56s\n",
      "epoch 40 | loss: 0.87896 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.35116 |  0:00:57s\n",
      "epoch 41 | loss: 0.87089 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.33708 |  0:00:58s\n",
      "epoch 42 | loss: 0.87627 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.33859 |  0:01:00s\n",
      "epoch 43 | loss: 0.88118 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.36152 |  0:01:01s\n",
      "epoch 44 | loss: 0.87372 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.35524 |  0:01:02s\n",
      "epoch 45 | loss: 0.87198 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.33776 |  0:01:04s\n",
      "epoch 46 | loss: 0.86659 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.34116 |  0:01:05s\n",
      "epoch 47 | loss: 0.87638 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.33426 |  0:01:06s\n",
      "epoch 48 | loss: 0.86553 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.37312 |  0:01:08s\n",
      "epoch 49 | loss: 0.86159 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.33883 |  0:01:09s\n",
      "epoch 50 | loss: 0.86743 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.39797 |  0:01:10s\n",
      "epoch 51 | loss: 0.8665  | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.35629 |  0:01:12s\n",
      "epoch 52 | loss: 0.86873 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.39743 |  0:01:13s\n",
      "epoch 53 | loss: 0.86826 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.34199 |  0:01:14s\n",
      "epoch 54 | loss: 0.84267 | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.3676  |  0:01:16s\n",
      "epoch 55 | loss: 0.86381 | val_0_accuracy: 0.5303  | val_0_balanced_accuracy: 0.35179 |  0:01:17s\n",
      "epoch 56 | loss: 0.86352 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.38846 |  0:01:18s\n",
      "epoch 57 | loss: 0.86078 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.3631  |  0:01:20s\n",
      "epoch 58 | loss: 0.85738 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.37528 |  0:01:21s\n",
      "epoch 59 | loss: 0.86769 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.35758 |  0:01:22s\n",
      "epoch 60 | loss: 0.84949 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.33496 |  0:01:24s\n",
      "epoch 61 | loss: 0.85052 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.38133 |  0:01:25s\n",
      "epoch 62 | loss: 0.85112 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.30928 |  0:01:27s\n",
      "epoch 63 | loss: 0.853   | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.33158 |  0:01:28s\n",
      "epoch 64 | loss: 0.84948 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.31687 |  0:01:29s\n",
      "epoch 65 | loss: 0.84869 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.34012 |  0:01:31s\n",
      "epoch 66 | loss: 0.85824 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.33773 |  0:01:32s\n",
      "epoch 67 | loss: 0.86082 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.34873 |  0:01:34s\n",
      "epoch 68 | loss: 0.85261 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.36268 |  0:01:35s\n",
      "epoch 69 | loss: 0.85463 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.36908 |  0:01:36s\n",
      "epoch 70 | loss: 0.85242 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38102 |  0:01:38s\n",
      "epoch 71 | loss: 0.84952 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.37217 |  0:01:39s\n",
      "epoch 72 | loss: 0.8541  | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.38741 |  0:01:40s\n",
      "epoch 73 | loss: 0.8452  | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.40106 |  0:01:42s\n",
      "epoch 74 | loss: 0.85344 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.43858 |  0:01:43s\n",
      "epoch 75 | loss: 0.86817 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.38442 |  0:01:45s\n",
      "epoch 76 | loss: 0.84433 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.36322 |  0:01:46s\n",
      "epoch 77 | loss: 0.85126 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.3372  |  0:01:47s\n",
      "epoch 78 | loss: 0.85519 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.37431 |  0:01:49s\n",
      "epoch 79 | loss: 0.84258 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.33625 |  0:01:50s\n",
      "epoch 80 | loss: 0.84682 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.34372 |  0:01:51s\n",
      "epoch 81 | loss: 0.84691 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.40064 |  0:01:53s\n",
      "epoch 82 | loss: 0.84793 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.37312 |  0:01:54s\n",
      "epoch 83 | loss: 0.84797 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.3647  |  0:01:55s\n",
      "epoch 84 | loss: 0.84553 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.34423 |  0:01:57s\n",
      "epoch 85 | loss: 0.83671 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.32973 |  0:01:58s\n",
      "epoch 86 | loss: 0.84168 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.3647  |  0:01:59s\n",
      "epoch 87 | loss: 0.84455 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.2917  |  0:02:01s\n",
      "epoch 88 | loss: 0.83688 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.31133 |  0:02:02s\n",
      "epoch 89 | loss: 0.84652 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.28502 |  0:02:03s\n",
      "epoch 90 | loss: 0.84683 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.33496 |  0:02:05s\n",
      "epoch 91 | loss: 0.83062 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.32687 |  0:02:06s\n",
      "epoch 92 | loss: 0.83105 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.33679 |  0:02:07s\n",
      "epoch 93 | loss: 0.83531 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.34277 |  0:02:09s\n",
      "epoch 94 | loss: 0.8373  | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.32378 |  0:02:10s\n",
      "epoch 95 | loss: 0.82821 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.33465 |  0:02:12s\n",
      "epoch 96 | loss: 0.82064 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.34586 |  0:02:13s\n",
      "epoch 97 | loss: 0.82465 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.33338 |  0:02:14s\n",
      "epoch 98 | loss: 0.81676 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.31247 |  0:02:16s\n",
      "epoch 99 | loss: 0.83301 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.32071 |  0:02:17s\n",
      "epoch 100| loss: 0.8267  | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.3596  |  0:02:18s\n",
      "epoch 101| loss: 0.82905 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.35736 |  0:02:20s\n",
      "epoch 102| loss: 0.82235 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.3336  |  0:02:21s\n",
      "epoch 103| loss: 0.82583 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.31911 |  0:02:22s\n",
      "epoch 104| loss: 0.82788 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.31899 |  0:02:24s\n",
      "epoch 105| loss: 0.8115  | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.33795 |  0:02:25s\n",
      "epoch 106| loss: 0.81952 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.35597 |  0:02:27s\n",
      "epoch 107| loss: 0.81677 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.33572 |  0:02:28s\n",
      "epoch 108| loss: 0.8131  | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.35874 |  0:02:29s\n",
      "epoch 109| loss: 0.8202  | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.34114 |  0:02:30s\n",
      "epoch 110| loss: 0.82148 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.32986 |  0:02:32s\n",
      "epoch 111| loss: 0.8166  | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.3791  |  0:02:33s\n",
      "epoch 112| loss: 0.83129 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.39391 |  0:02:35s\n",
      "epoch 113| loss: 0.82163 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.34744 |  0:02:36s\n",
      "epoch 114| loss: 0.81629 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.36779 |  0:02:37s\n",
      "epoch 115| loss: 0.82907 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.34914 |  0:02:39s\n",
      "epoch 116| loss: 0.82397 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.34595 |  0:02:40s\n",
      "epoch 117| loss: 0.81809 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.34617 |  0:02:42s\n",
      "epoch 118| loss: 0.82118 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.35459 |  0:02:43s\n",
      "epoch 119| loss: 0.81884 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.32507 |  0:02:44s\n",
      "epoch 120| loss: 0.82559 | val_0_accuracy: 0.55303 | val_0_balanced_accuracy: 0.39271 |  0:02:46s\n",
      "epoch 121| loss: 0.81034 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.40169 |  0:02:47s\n",
      "epoch 122| loss: 0.80846 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.36619 |  0:02:48s\n",
      "epoch 123| loss: 0.81223 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.36373 |  0:02:50s\n",
      "epoch 124| loss: 0.81408 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.37705 |  0:02:51s\n",
      "epoch 125| loss: 0.82453 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.33127 |  0:02:52s\n",
      "epoch 126| loss: 0.82312 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.34924 |  0:02:54s\n",
      "epoch 127| loss: 0.81019 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.39391 |  0:02:55s\n",
      "epoch 128| loss: 0.8236  | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.40254 |  0:02:56s\n",
      "epoch 129| loss: 0.81728 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.36908 |  0:02:58s\n",
      "epoch 130| loss: 0.8067  | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.37268 |  0:02:59s\n",
      "epoch 131| loss: 0.81892 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.34201 |  0:03:01s\n",
      "epoch 132| loss: 0.81596 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.39101 |  0:03:02s\n",
      "epoch 133| loss: 0.81445 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.36237 |  0:03:03s\n",
      "epoch 134| loss: 0.79828 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.34498 |  0:03:05s\n",
      "epoch 135| loss: 0.80932 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.3567  |  0:03:06s\n",
      "epoch 136| loss: 0.8137  | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.3548  |  0:03:07s\n",
      "epoch 137| loss: 0.80622 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.38559 |  0:03:09s\n",
      "epoch 138| loss: 0.80994 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.37343 |  0:03:10s\n",
      "epoch 139| loss: 0.80706 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.38698 |  0:03:12s\n",
      "epoch 140| loss: 0.81165 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.42544 |  0:03:13s\n",
      "epoch 141| loss: 0.8118  | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.38133 |  0:03:14s\n",
      "epoch 142| loss: 0.80278 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.35425 |  0:03:15s\n",
      "epoch 143| loss: 0.81207 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.36512 |  0:03:17s\n",
      "epoch 144| loss: 0.81164 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.35916 |  0:03:18s\n",
      "epoch 145| loss: 0.81096 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.34819 |  0:03:19s\n",
      "epoch 146| loss: 0.81014 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.35191 |  0:03:21s\n",
      "epoch 147| loss: 0.80646 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.38581 |  0:03:22s\n",
      "epoch 148| loss: 0.80363 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.37399 |  0:03:23s\n",
      "epoch 149| loss: 0.79461 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.37749 |  0:03:25s\n",
      "epoch 150| loss: 0.80198 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.38389 |  0:03:26s\n",
      "epoch 151| loss: 0.79845 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38357 |  0:03:27s\n",
      "epoch 152| loss: 0.79764 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.39006 |  0:03:29s\n",
      "epoch 153| loss: 0.79904 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.39123 |  0:03:30s\n",
      "epoch 154| loss: 0.79773 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.40723 |  0:03:31s\n",
      "epoch 155| loss: 0.7954  | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.36949 |  0:03:32s\n",
      "epoch 156| loss: 0.79836 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.37195 |  0:03:34s\n",
      "epoch 157| loss: 0.78932 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.33966 |  0:03:35s\n",
      "epoch 158| loss: 0.80661 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.39923 |  0:03:37s\n",
      "epoch 159| loss: 0.80347 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.40349 |  0:03:38s\n",
      "epoch 160| loss: 0.79769 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.38197 |  0:03:39s\n",
      "epoch 161| loss: 0.78465 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.39476 |  0:03:41s\n",
      "epoch 162| loss: 0.80522 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.36682 |  0:03:42s\n",
      "epoch 163| loss: 0.80214 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.3549  |  0:03:43s\n",
      "epoch 164| loss: 0.79418 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.39019 |  0:03:45s\n",
      "epoch 165| loss: 0.79315 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38219 |  0:03:46s\n",
      "epoch 166| loss: 0.80242 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.32998 |  0:03:47s\n",
      "epoch 167| loss: 0.80662 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.42119 |  0:03:49s\n",
      "epoch 168| loss: 0.7929  | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.40402 |  0:03:50s\n",
      "epoch 169| loss: 0.80034 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38357 |  0:03:52s\n",
      "epoch 170| loss: 0.79494 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.34702 |  0:03:53s\n",
      "epoch 171| loss: 0.78861 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.41438 |  0:03:54s\n",
      "epoch 172| loss: 0.8087  | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.36726 |  0:03:56s\n",
      "epoch 173| loss: 0.79119 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.41394 |  0:03:57s\n",
      "epoch 174| loss: 0.79163 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.36003 |  0:03:58s\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 74 and best_val_0_balanced_accuracy = 0.43858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [142 223  44]\n",
      "Распределение истинных классов: [219 126  64]\n",
      "Accuracy: 0.4034\n",
      "F1-score: 0.4080\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.59      0.38      0.47       219\n",
      "          c1       0.30      0.53      0.38       126\n",
      "          c2       0.32      0.22      0.26        64\n",
      "\n",
      "    accuracy                           0.40       409\n",
      "   macro avg       0.40      0.38      0.37       409\n",
      "weighted avg       0.46      0.40      0.41       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " high_low       0.149078\n",
      "open_diff_2    0.093540\n",
      "rsi            0.068282\n",
      "atr            0.058775\n",
      "open           0.057926\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.ffill(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.006\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление признаков\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "bb = ta.bbands(df['close'], length=20)\n",
    "df['bb_width'] = (bb['BBU_20_2.0'] - bb['BBL_20_2.0']) / df['close']\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "\n",
    "# Исправление MACD\n",
    "macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "print(\"Доступные столбцы MACD:\", macd.columns)  # Проверка имен столбцов\n",
    "df['macd'] = macd['MACD_12_26_9']\n",
    "df['macd_signal'] = macd['MACDs_12_26_9']  # Исправлено\n",
    "df['adx'] = ta.adx(df['high'], df['low'], df['close'], length=14)['ADX_14']\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr', \n",
    "                'rsi', 'bb_width', 'volume_change', 'macd', 'macd_signal', 'adx']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf и NaN\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.01, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling с SMOTE\n",
    "smote = SMOTE(sampling_strategy={1: 4000, 2: 3000}, random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train (после SMOTE):\", np.bincount(y_train_res))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.0, 1: 1.2, 2: 2.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=128, n_a=128, n_steps=7, gamma=1.5,\n",
    "    lambda_sparse=0.001, optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=100,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=256,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c618288-167a-4c9b-82a7-97bf39f82f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение y_train (после SMOTE): [5519 3000 3000]\n",
      "Распределение y_val: [149  92  23]\n",
      "Распределение y_test: [219 126  64]\n",
      "Веса классов: {0: 1.0, 1: 1.0, 2: 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.4802  | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.32346 |  0:00:01s\n",
      "epoch 1  | loss: 1.85568 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.33591 |  0:00:02s\n",
      "epoch 2  | loss: 1.71389 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.38878 |  0:00:03s\n",
      "epoch 3  | loss: 1.51232 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.3292  |  0:00:04s\n",
      "epoch 4  | loss: 1.6165  | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.33294 |  0:00:05s\n",
      "epoch 5  | loss: 1.53797 | val_0_accuracy: 0.40152 | val_0_balanced_accuracy: 0.37239 |  0:00:06s\n",
      "epoch 6  | loss: 1.64364 | val_0_accuracy: 0.30303 | val_0_balanced_accuracy: 0.25477 |  0:00:07s\n",
      "epoch 7  | loss: 1.71464 | val_0_accuracy: 0.39394 | val_0_balanced_accuracy: 0.30248 |  0:00:08s\n",
      "epoch 8  | loss: 1.67663 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.44397 |  0:00:09s\n",
      "epoch 9  | loss: 1.65991 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.36341 |  0:00:10s\n",
      "epoch 10 | loss: 1.68983 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.40604 |  0:00:11s\n",
      "epoch 11 | loss: 1.37058 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.37504 |  0:00:12s\n",
      "epoch 12 | loss: 1.39163 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.3843  |  0:00:13s\n",
      "epoch 13 | loss: 1.45602 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.41681 |  0:00:14s\n",
      "epoch 14 | loss: 1.4545  | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.44293 |  0:00:15s\n",
      "epoch 15 | loss: 1.40006 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.428   |  0:00:17s\n",
      "epoch 16 | loss: 1.3352  | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.36767 |  0:00:18s\n",
      "epoch 17 | loss: 1.38325 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.347   |  0:00:19s\n",
      "epoch 18 | loss: 1.47317 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.37538 |  0:00:20s\n",
      "epoch 19 | loss: 1.33686 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.37856 |  0:00:21s\n",
      "epoch 20 | loss: 1.28804 | val_0_accuracy: 0.38258 | val_0_balanced_accuracy: 0.29161 |  0:00:22s\n",
      "epoch 21 | loss: 1.27396 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.34221 |  0:00:23s\n",
      "epoch 22 | loss: 1.25482 | val_0_accuracy: 0.39015 | val_0_balanced_accuracy: 0.33978 |  0:00:24s\n",
      "epoch 23 | loss: 1.44873 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.31779 |  0:00:25s\n",
      "epoch 24 | loss: 1.31491 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.36597 |  0:00:26s\n",
      "epoch 25 | loss: 1.3308  | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.31269 |  0:00:27s\n",
      "epoch 26 | loss: 1.25858 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.30544 |  0:00:28s\n",
      "epoch 27 | loss: 1.44211 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.33462 |  0:00:29s\n",
      "epoch 28 | loss: 1.41667 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.29586 |  0:00:30s\n",
      "epoch 29 | loss: 1.25797 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.37173 |  0:00:31s\n",
      "epoch 30 | loss: 1.06603 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.35106 |  0:00:33s\n",
      "epoch 31 | loss: 1.01715 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.32283 |  0:00:34s\n",
      "epoch 32 | loss: 1.22218 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.33871 |  0:00:35s\n",
      "epoch 33 | loss: 1.1517  | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.32378 |  0:00:36s\n",
      "epoch 34 | loss: 1.06152 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.3274  |  0:00:37s\n",
      "epoch 35 | loss: 1.02986 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.37504 |  0:00:38s\n",
      "epoch 36 | loss: 1.05499 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.36825 |  0:00:39s\n",
      "epoch 37 | loss: 1.08373 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.34829 |  0:00:40s\n",
      "epoch 38 | loss: 0.99255 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.33915 |  0:00:41s\n",
      "epoch 39 | loss: 1.01117 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.30802 |  0:00:42s\n",
      "epoch 40 | loss: 0.96199 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.37175 |  0:00:43s\n",
      "epoch 41 | loss: 0.9258  | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.30693 |  0:00:44s\n",
      "epoch 42 | loss: 1.02038 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.30865 |  0:00:46s\n",
      "epoch 43 | loss: 1.00881 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.30831 |  0:00:47s\n",
      "epoch 44 | loss: 0.96404 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.34491 |  0:00:48s\n",
      "epoch 45 | loss: 0.99544 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.41256 |  0:00:49s\n",
      "epoch 46 | loss: 0.90737 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38313 |  0:00:50s\n",
      "epoch 47 | loss: 0.87721 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.37523 |  0:00:51s\n",
      "epoch 48 | loss: 0.90462 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.38953 |  0:00:52s\n",
      "epoch 49 | loss: 0.93058 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.38676 |  0:00:53s\n",
      "epoch 50 | loss: 0.89497 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.31407 |  0:00:54s\n",
      "epoch 51 | loss: 0.93501 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.33219 |  0:00:55s\n",
      "epoch 52 | loss: 0.90975 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.3175  |  0:00:56s\n",
      "epoch 53 | loss: 1.01946 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.31193 |  0:00:57s\n",
      "epoch 54 | loss: 0.93212 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.34245 |  0:00:58s\n",
      "epoch 55 | loss: 0.94723 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.37611 |  0:00:59s\n",
      "epoch 56 | loss: 0.90486 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.31108 |  0:01:00s\n",
      "epoch 57 | loss: 0.88477 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.32558 |  0:01:01s\n",
      "epoch 58 | loss: 0.84625 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.36439 |  0:01:02s\n",
      "epoch 59 | loss: 0.86633 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.32699 |  0:01:04s\n",
      "epoch 60 | loss: 0.86264 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.31184 |  0:01:05s\n",
      "epoch 61 | loss: 0.87156 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.34328 |  0:01:06s\n",
      "epoch 62 | loss: 0.80459 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.32879 |  0:01:07s\n",
      "epoch 63 | loss: 0.85268 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.33786 |  0:01:08s\n",
      "epoch 64 | loss: 0.87248 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.32888 |  0:01:09s\n",
      "epoch 65 | loss: 0.83708 | val_0_accuracy: 0.42803 | val_0_balanced_accuracy: 0.31889 |  0:01:10s\n",
      "epoch 66 | loss: 0.83769 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.33795 |  0:01:11s\n",
      "epoch 67 | loss: 0.82908 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.41681 |  0:01:13s\n",
      "epoch 68 | loss: 0.847   | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.33499 |  0:01:14s\n",
      "epoch 69 | loss: 0.82707 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.36896 |  0:01:15s\n",
      "epoch 70 | loss: 0.83754 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.4005  |  0:01:16s\n",
      "epoch 71 | loss: 0.84824 | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.35884 |  0:01:17s\n",
      "epoch 72 | loss: 0.84742 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.32409 |  0:01:18s\n",
      "epoch 73 | loss: 0.80769 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.34296 |  0:01:19s\n",
      "epoch 74 | loss: 0.82496 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.38036 |  0:01:21s\n",
      "epoch 75 | loss: 0.85219 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.39558 |  0:01:22s\n",
      "epoch 76 | loss: 0.81797 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.38048 |  0:01:23s\n",
      "epoch 77 | loss: 0.81925 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.38807 |  0:01:24s\n",
      "epoch 78 | loss: 0.80996 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.35904 |  0:01:25s\n",
      "epoch 79 | loss: 0.83142 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.41895 |  0:01:26s\n",
      "epoch 80 | loss: 0.83047 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.3301  |  0:01:27s\n",
      "epoch 81 | loss: 0.84938 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.34007 |  0:01:28s\n",
      "epoch 82 | loss: 0.83834 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.39709 |  0:01:29s\n",
      "epoch 83 | loss: 0.84053 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.36704 |  0:01:30s\n",
      "epoch 84 | loss: 0.82572 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.35065 |  0:01:31s\n",
      "epoch 85 | loss: 0.83299 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.36276 |  0:01:32s\n",
      "epoch 86 | loss: 0.81887 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.33805 |  0:01:33s\n",
      "epoch 87 | loss: 0.80866 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.34646 |  0:01:34s\n",
      "epoch 88 | loss: 0.80666 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.34809 |  0:01:35s\n",
      "epoch 89 | loss: 0.84322 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.36278 |  0:01:36s\n",
      "epoch 90 | loss: 0.79737 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.34605 |  0:01:38s\n",
      "epoch 91 | loss: 0.80579 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.31514 |  0:01:39s\n",
      "epoch 92 | loss: 0.78911 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.32932 |  0:01:40s\n",
      "epoch 93 | loss: 0.7917  | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.35072 |  0:01:41s\n",
      "epoch 94 | loss: 0.7854  | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.33175 |  0:01:42s\n",
      "epoch 95 | loss: 0.80845 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.33144 |  0:01:43s\n",
      "epoch 96 | loss: 0.79729 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.33987 |  0:01:44s\n",
      "epoch 97 | loss: 0.78787 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.32324 |  0:01:45s\n",
      "epoch 98 | loss: 0.78436 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.32973 |  0:01:46s\n",
      "epoch 99 | loss: 0.79934 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.33039 |  0:01:47s\n",
      "epoch 100| loss: 0.81032 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.35342 |  0:01:48s\n",
      "epoch 101| loss: 0.78386 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.36353 |  0:01:49s\n",
      "epoch 102| loss: 0.78425 | val_0_accuracy: 0.39015 | val_0_balanced_accuracy: 0.3141  |  0:01:50s\n",
      "epoch 103| loss: 0.81376 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.3418  |  0:01:51s\n",
      "epoch 104| loss: 0.82626 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.38335 |  0:01:52s\n",
      "epoch 105| loss: 0.78723 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.31483 |  0:01:53s\n",
      "epoch 106| loss: 0.78414 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.32314 |  0:01:54s\n",
      "epoch 107| loss: 0.77059 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.32453 |  0:01:55s\n",
      "epoch 108| loss: 0.78818 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.34892 |  0:01:56s\n",
      "\n",
      "Early stopping occurred at epoch 108 with best_epoch = 8 and best_val_0_balanced_accuracy = 0.44397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [288  54  67]\n",
      "Распределение истинных классов: [219 126  64]\n",
      "Accuracy: 0.4768\n",
      "F1-score: 0.4420\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.56      0.74      0.64       219\n",
      "          c1       0.37      0.16      0.22       126\n",
      "          c2       0.21      0.22      0.21        64\n",
      "\n",
      "    accuracy                           0.48       409\n",
      "   macro avg       0.38      0.37      0.36       409\n",
      "weighted avg       0.45      0.48      0.44       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " stoch_d         0.085833\n",
      "high_low        0.064700\n",
      "ema_long        0.055856\n",
      "bb_width        0.052458\n",
      "close_diff_2    0.049132\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.ffill(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.006\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление признаков\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "bb = ta.bbands(df['close'], length=20)\n",
    "df['bb_width'] = (bb['BBU_20_2.0'] - bb['BBL_20_2.0']) / df['close']\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "df['macd'] = macd['MACD_12_26_9']\n",
    "df['macd_signal'] = macd['MACDs_12_26_9']\n",
    "df['adx'] = ta.adx(df['high'], df['low'], df['close'], length=14)['ADX_14']\n",
    "df['ema_short'] = ta.ema(df['close'], length=12)\n",
    "df['ema_long'] = ta.ema(df['close'], length=50)\n",
    "df['atr_norm'] = df['atr'] / df['close']\n",
    "df['log_close'] = np.log1p(df['close'])\n",
    "df['ema_ratio'] = df['ema_short'] / df['ema_long']\n",
    "stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3, smooth_k=3)\n",
    "df['stoch_k'] = stoch['STOCHk_14_3_3']\n",
    "df['stoch_d'] = stoch['STOCHd_14_3_3']\n",
    "df['obv'] = ta.obv(df['close'], df['volume'])\n",
    "df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20)\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr', \n",
    "                'rsi', 'bb_width', 'volume_change', 'macd', 'macd_signal', 'adx',\n",
    "                'ema_short', 'ema_long', 'atr_norm', 'log_close', 'ema_ratio',\n",
    "                'stoch_k', 'stoch_d', 'obv', 'cci']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf и NaN\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.01, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling с SMOTE\n",
    "smote = SMOTE(sampling_strategy={1: 3000, 2: 3000}, random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train (после SMOTE):\", np.bincount(y_train_res))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.0, 1: 1.0, 2: 2.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=512, n_a=512, n_steps=7, gamma=1.5,\n",
    "    lambda_sparse=0.0005, \n",
    "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={\"step_size\": 30, \"gamma\": 0.85},\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=100,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=512,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c5e3332-4645-47bb-b929-c65a13910a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение y_train (после SMOTE): [5519 4000 3000]\n",
      "Распределение y_val: [149  92  23]\n",
      "Распределение y_test: [219 126  64]\n",
      "Веса классов: {0: 1.0, 1: 1.2, 2: 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.95854 | val_0_accuracy: 0.29924 | val_0_balanced_accuracy: 0.34503 |  0:00:01s\n",
      "epoch 1  | loss: 2.14501 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.3416  |  0:00:02s\n",
      "epoch 2  | loss: 1.79233 | val_0_accuracy: 0.32955 | val_0_balanced_accuracy: 0.32499 |  0:00:03s\n",
      "epoch 3  | loss: 1.57092 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.36623 |  0:00:03s\n",
      "epoch 4  | loss: 1.42925 | val_0_accuracy: 0.37879 | val_0_balanced_accuracy: 0.36998 |  0:00:04s\n",
      "epoch 5  | loss: 1.28766 | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.38977 |  0:00:05s\n",
      "epoch 6  | loss: 1.26701 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.34352 |  0:00:06s\n",
      "epoch 7  | loss: 1.32903 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.42301 |  0:00:07s\n",
      "epoch 8  | loss: 1.31074 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.36825 |  0:00:08s\n",
      "epoch 9  | loss: 1.29672 | val_0_accuracy: 0.38258 | val_0_balanced_accuracy: 0.31933 |  0:00:09s\n",
      "epoch 10 | loss: 1.31629 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.34189 |  0:00:10s\n",
      "epoch 11 | loss: 1.38073 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.39733 |  0:00:11s\n",
      "epoch 12 | loss: 1.17451 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.36067 |  0:00:12s\n",
      "epoch 13 | loss: 1.19919 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.26007 |  0:00:12s\n",
      "epoch 14 | loss: 1.21817 | val_0_accuracy: 0.36364 | val_0_balanced_accuracy: 0.3139  |  0:00:13s\n",
      "epoch 15 | loss: 1.19039 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.34875 |  0:00:14s\n",
      "epoch 16 | loss: 1.26008 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.36687 |  0:00:15s\n",
      "epoch 17 | loss: 1.30776 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.38571 |  0:00:16s\n",
      "epoch 18 | loss: 1.33638 | val_0_accuracy: 0.38258 | val_0_balanced_accuracy: 0.31305 |  0:00:17s\n",
      "epoch 19 | loss: 1.2216  | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.34629 |  0:00:18s\n",
      "epoch 20 | loss: 1.15977 | val_0_accuracy: 0.35985 | val_0_balanced_accuracy: 0.39746 |  0:00:19s\n",
      "epoch 21 | loss: 1.09987 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.3513  |  0:00:20s\n",
      "epoch 22 | loss: 1.16162 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.3984  |  0:00:21s\n",
      "epoch 23 | loss: 1.17188 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.35215 |  0:00:22s\n",
      "epoch 24 | loss: 1.12373 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.45497 |  0:00:22s\n",
      "epoch 25 | loss: 1.11778 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.31407 |  0:00:23s\n",
      "epoch 26 | loss: 1.06383 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.34595 |  0:00:24s\n",
      "epoch 27 | loss: 0.98133 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.36757 |  0:00:25s\n",
      "epoch 28 | loss: 1.09191 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.34573 |  0:00:26s\n",
      "epoch 29 | loss: 1.0911  | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.39371 |  0:00:27s\n",
      "epoch 30 | loss: 1.10775 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.37474 |  0:00:28s\n",
      "epoch 31 | loss: 1.06827 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.42352 |  0:00:29s\n",
      "epoch 32 | loss: 1.01064 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.34065 |  0:00:29s\n",
      "epoch 33 | loss: 1.05463 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.3479  |  0:00:30s\n",
      "epoch 34 | loss: 0.99275 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.35191 |  0:00:31s\n",
      "epoch 35 | loss: 1.01894 | val_0_accuracy: 0.39015 | val_0_balanced_accuracy: 0.28959 |  0:00:32s\n",
      "epoch 36 | loss: 0.97902 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.36013 |  0:00:33s\n",
      "epoch 37 | loss: 0.9606  | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.40273 |  0:00:34s\n",
      "epoch 38 | loss: 0.99677 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.31507 |  0:00:35s\n",
      "epoch 39 | loss: 0.96749 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.28385 |  0:00:36s\n",
      "epoch 40 | loss: 0.97876 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.34702 |  0:00:37s\n",
      "epoch 41 | loss: 1.02033 | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.30887 |  0:00:37s\n",
      "epoch 42 | loss: 1.03159 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.36524 |  0:00:38s\n",
      "epoch 43 | loss: 0.98293 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.32144 |  0:00:39s\n",
      "epoch 44 | loss: 0.94499 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.3061  |  0:00:40s\n",
      "epoch 45 | loss: 1.00087 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.32762 |  0:00:41s\n",
      "epoch 46 | loss: 0.96739 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.34936 |  0:00:42s\n",
      "epoch 47 | loss: 0.9657  | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.35617 |  0:00:43s\n",
      "epoch 48 | loss: 1.02213 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.32122 |  0:00:43s\n",
      "epoch 49 | loss: 0.96743 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.36259 |  0:00:44s\n",
      "epoch 50 | loss: 0.91564 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.41747 |  0:00:45s\n",
      "epoch 51 | loss: 0.92482 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.35014 |  0:00:46s\n",
      "epoch 52 | loss: 0.9956  | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.36801 |  0:00:47s\n",
      "epoch 53 | loss: 0.93198 | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.32516 |  0:00:48s\n",
      "epoch 54 | loss: 0.89994 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.36621 |  0:00:49s\n",
      "epoch 55 | loss: 0.92206 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.38561 |  0:00:50s\n",
      "epoch 56 | loss: 0.96961 | val_0_accuracy: 0.44318 | val_0_balanced_accuracy: 0.31303 |  0:00:50s\n",
      "epoch 57 | loss: 0.96758 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.43588 |  0:00:51s\n",
      "epoch 58 | loss: 0.93582 | val_0_accuracy: 0.36742 | val_0_balanced_accuracy: 0.40864 |  0:00:52s\n",
      "epoch 59 | loss: 0.90107 | val_0_accuracy: 0.34848 | val_0_balanced_accuracy: 0.34566 |  0:00:53s\n",
      "epoch 60 | loss: 0.87058 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.3516  |  0:00:54s\n",
      "epoch 61 | loss: 0.91204 | val_0_accuracy: 0.38636 | val_0_balanced_accuracy: 0.36293 |  0:00:55s\n",
      "epoch 62 | loss: 0.89377 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.37995 |  0:00:56s\n",
      "epoch 63 | loss: 0.91557 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.36198 |  0:00:57s\n",
      "epoch 64 | loss: 0.91279 | val_0_accuracy: 0.41667 | val_0_balanced_accuracy: 0.35099 |  0:00:57s\n",
      "epoch 65 | loss: 0.89734 | val_0_accuracy: 0.39394 | val_0_balanced_accuracy: 0.31838 |  0:00:58s\n",
      "epoch 66 | loss: 0.90643 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.31741 |  0:00:59s\n",
      "epoch 67 | loss: 0.91615 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.32349 |  0:01:00s\n",
      "epoch 68 | loss: 0.9588  | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.28959 |  0:01:01s\n",
      "epoch 69 | loss: 0.95262 | val_0_accuracy: 0.40909 | val_0_balanced_accuracy: 0.32966 |  0:01:02s\n",
      "epoch 70 | loss: 0.92132 | val_0_accuracy: 0.41288 | val_0_balanced_accuracy: 0.32774 |  0:01:03s\n",
      "epoch 71 | loss: 0.89313 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.34938 |  0:01:04s\n",
      "epoch 72 | loss: 0.8989  | val_0_accuracy: 0.37879 | val_0_balanced_accuracy: 0.29236 |  0:01:04s\n",
      "epoch 73 | loss: 0.9175  | val_0_accuracy: 0.46212 | val_0_balanced_accuracy: 0.34479 |  0:01:05s\n",
      "epoch 74 | loss: 0.90921 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.30972 |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 24 and best_val_0_balanced_accuracy = 0.45497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [210 105  94]\n",
      "Распределение истинных классов: [219 126  64]\n",
      "Accuracy: 0.3985\n",
      "F1-score: 0.4031\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.56      0.53      0.55       219\n",
      "          c1       0.28      0.23      0.25       126\n",
      "          c2       0.18      0.27      0.22        64\n",
      "\n",
      "    accuracy                           0.40       409\n",
      "   macro avg       0.34      0.34      0.34       409\n",
      "weighted avg       0.41      0.40      0.40       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " close       0.074021\n",
      "low         0.062543\n",
      "adx         0.055790\n",
      "high        0.053958\n",
      "high_low    0.047183\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.ffill(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.006\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление признаков\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "bb = ta.bbands(df['close'], length=20)\n",
    "df['bb_width'] = (bb['BBU_20_2.0'] - bb['BBL_20_2.0']) / df['close']\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "df['macd'] = macd['MACD_12_26_9']\n",
    "df['macd_signal'] = macd['MACDs_12_26_9']\n",
    "df['adx'] = ta.adx(df['high'], df['low'], df['close'], length=14)['ADX_14']\n",
    "df['ema_short'] = ta.ema(df['close'], length=12)\n",
    "df['ema_long'] = ta.ema(df['close'], length=50)\n",
    "df['atr_norm'] = df['atr'] / df['close']\n",
    "df['ema_ratio'] = df['ema_short'] / df['ema_long']\n",
    "stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3, smooth_k=3)\n",
    "df['stoch_d'] = stoch['STOCHd_14_3_3']\n",
    "df['obv'] = ta.obv(df['close'], df['volume'])\n",
    "df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20)\n",
    "df['roc'] = ta.roc(df['close'], length=10)\n",
    "df['volume_diff_3'] = df['volume'].diff(3)\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr', \n",
    "                'rsi', 'bb_width', 'volume_change', 'macd', 'macd_signal', 'adx',\n",
    "                'ema_short', 'ema_long', 'atr_norm', 'ema_ratio', 'stoch_d', \n",
    "                'obv', 'cci', 'roc', 'volume_diff_3']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf и NaN\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.02, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling с SMOTE\n",
    "smote = SMOTE(sampling_strategy={1: 4000, 2: 3000}, random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train (после SMOTE):\", np.bincount(y_train_res))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.0, 1: 1.2, 2: 2.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=256, n_a=256, n_steps=8, gamma=1.5,\n",
    "    lambda_sparse=0.0005, \n",
    "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=512,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73b8df20-80c0-447d-8c12-4403b8561115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение y_train (после SMOTE): [5519 3500 3500]\n",
      "Распределение y_val: [149  92  23]\n",
      "Распределение y_test: [219 126  64]\n",
      "Веса классов: {0: 1.2, 1: 1.2, 2: 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.40339 | val_0_accuracy: 0.375   | val_0_balanced_accuracy: 0.32871 |  0:00:00s\n",
      "epoch 1  | loss: 1.61511 | val_0_accuracy: 0.5303  | val_0_balanced_accuracy: 0.37674 |  0:00:01s\n",
      "epoch 2  | loss: 1.3987  | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.33657 |  0:00:02s\n",
      "epoch 3  | loss: 1.25687 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.32431 |  0:00:02s\n",
      "epoch 4  | loss: 1.15569 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.36512 |  0:00:03s\n",
      "epoch 5  | loss: 1.10847 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.33316 |  0:00:03s\n",
      "epoch 6  | loss: 1.02615 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.34413 |  0:00:04s\n",
      "epoch 7  | loss: 1.04261 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.36884 |  0:00:05s\n",
      "epoch 8  | loss: 1.00917 | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.34425 |  0:00:05s\n",
      "epoch 9  | loss: 1.00953 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.31313 |  0:00:06s\n",
      "epoch 10 | loss: 0.97089 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.37664 |  0:00:07s\n",
      "epoch 11 | loss: 0.97322 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.38206 |  0:00:08s\n",
      "epoch 12 | loss: 0.94515 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.44035 |  0:00:08s\n",
      "epoch 13 | loss: 0.98023 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.39337 |  0:00:09s\n",
      "epoch 14 | loss: 0.96252 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.36268 |  0:00:10s\n",
      "epoch 15 | loss: 0.91945 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.4084  |  0:00:10s\n",
      "epoch 16 | loss: 0.93553 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.45304 |  0:00:11s\n",
      "epoch 17 | loss: 0.92341 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.34094 |  0:00:12s\n",
      "epoch 18 | loss: 0.91399 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.39337 |  0:00:13s\n",
      "epoch 19 | loss: 0.9007  | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.37834 |  0:00:13s\n",
      "epoch 20 | loss: 0.88934 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.39177 |  0:00:14s\n",
      "epoch 21 | loss: 0.89126 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.37674 |  0:00:14s\n",
      "epoch 22 | loss: 0.89996 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.36064 |  0:00:15s\n",
      "epoch 23 | loss: 0.89563 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.42734 |  0:00:16s\n",
      "epoch 24 | loss: 0.88206 | val_0_accuracy: 0.45455 | val_0_balanced_accuracy: 0.37248 |  0:00:16s\n",
      "epoch 25 | loss: 0.91819 | val_0_accuracy: 0.4053  | val_0_balanced_accuracy: 0.42897 |  0:00:17s\n",
      "epoch 26 | loss: 0.89141 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.35626 |  0:00:18s\n",
      "epoch 27 | loss: 0.87747 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.3566  |  0:00:18s\n",
      "epoch 28 | loss: 0.87002 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.313   |  0:00:19s\n",
      "epoch 29 | loss: 0.86228 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.33645 |  0:00:20s\n",
      "epoch 30 | loss: 0.85712 | val_0_accuracy: 0.41667 | val_0_balanced_accuracy: 0.35682 |  0:00:20s\n",
      "epoch 31 | loss: 0.85898 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.38228 |  0:00:21s\n",
      "epoch 32 | loss: 0.85933 | val_0_accuracy: 0.45076 | val_0_balanced_accuracy: 0.36237 |  0:00:22s\n",
      "epoch 33 | loss: 0.86144 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.32003 |  0:00:22s\n",
      "epoch 34 | loss: 0.85385 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.45358 |  0:00:23s\n",
      "epoch 35 | loss: 0.83859 | val_0_accuracy: 0.38258 | val_0_balanced_accuracy: 0.43218 |  0:00:24s\n",
      "epoch 36 | loss: 0.85456 | val_0_accuracy: 0.4697  | val_0_balanced_accuracy: 0.46007 |  0:00:24s\n",
      "epoch 37 | loss: 0.83729 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.38547 |  0:00:25s\n",
      "epoch 38 | loss: 0.85991 | val_0_accuracy: 0.43182 | val_0_balanced_accuracy: 0.41341 |  0:00:26s\n",
      "epoch 39 | loss: 0.84018 | val_0_accuracy: 0.55303 | val_0_balanced_accuracy: 0.39665 |  0:00:26s\n",
      "epoch 40 | loss: 0.81798 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.37195 |  0:00:27s\n",
      "epoch 41 | loss: 0.81959 | val_0_accuracy: 0.45833 | val_0_balanced_accuracy: 0.41565 |  0:00:28s\n",
      "epoch 42 | loss: 0.84153 | val_0_accuracy: 0.43561 | val_0_balanced_accuracy: 0.30834 |  0:00:28s\n",
      "epoch 43 | loss: 0.83235 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.45708 |  0:00:29s\n",
      "epoch 44 | loss: 0.84004 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.32876 |  0:00:30s\n",
      "epoch 45 | loss: 0.8121  | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.34943 |  0:00:30s\n",
      "epoch 46 | loss: 0.83862 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.42746 |  0:00:31s\n",
      "epoch 47 | loss: 0.82525 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.39473 |  0:00:32s\n",
      "epoch 48 | loss: 0.8367  | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.36458 |  0:00:32s\n",
      "epoch 49 | loss: 0.8379  | val_0_accuracy: 0.42045 | val_0_balanced_accuracy: 0.39969 |  0:00:33s\n",
      "epoch 50 | loss: 0.83566 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.42639 |  0:00:33s\n",
      "epoch 51 | loss: 0.81619 | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.33324 |  0:00:34s\n",
      "epoch 52 | loss: 0.81923 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.35551 |  0:00:35s\n",
      "epoch 53 | loss: 0.84257 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.3551  |  0:00:35s\n",
      "epoch 54 | loss: 0.82801 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.38333 |  0:00:36s\n",
      "epoch 55 | loss: 0.8091  | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.42671 |  0:00:37s\n",
      "epoch 56 | loss: 0.81701 | val_0_accuracy: 0.5303  | val_0_balanced_accuracy: 0.37491 |  0:00:37s\n",
      "epoch 57 | loss: 0.81842 | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.39974 |  0:00:38s\n",
      "epoch 58 | loss: 0.81673 | val_0_accuracy: 0.5303  | val_0_balanced_accuracy: 0.35434 |  0:00:38s\n",
      "epoch 59 | loss: 0.80395 | val_0_accuracy: 0.42424 | val_0_balanced_accuracy: 0.43461 |  0:00:39s\n",
      "epoch 60 | loss: 0.80465 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.35116 |  0:00:40s\n",
      "epoch 61 | loss: 0.81241 | val_0_accuracy: 0.48485 | val_0_balanced_accuracy: 0.37907 |  0:00:40s\n",
      "epoch 62 | loss: 0.80566 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.38985 |  0:00:41s\n",
      "epoch 63 | loss: 0.80218 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.37056 |  0:00:42s\n",
      "epoch 64 | loss: 0.80135 | val_0_accuracy: 0.43939 | val_0_balanced_accuracy: 0.38688 |  0:00:42s\n",
      "epoch 65 | loss: 0.80163 | val_0_accuracy: 0.52652 | val_0_balanced_accuracy: 0.35605 |  0:00:43s\n",
      "epoch 66 | loss: 0.79904 | val_0_accuracy: 0.55682 | val_0_balanced_accuracy: 0.3425  |  0:00:44s\n",
      "epoch 67 | loss: 0.79159 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.31789 |  0:00:44s\n",
      "epoch 68 | loss: 0.79925 | val_0_accuracy: 0.55303 | val_0_balanced_accuracy: 0.35529 |  0:00:45s\n",
      "epoch 69 | loss: 0.7904  | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.34401 |  0:00:45s\n",
      "epoch 70 | loss: 0.78141 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.37098 |  0:00:46s\n",
      "epoch 71 | loss: 0.79862 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.36521 |  0:00:47s\n",
      "epoch 72 | loss: 0.80641 | val_0_accuracy: 0.47727 | val_0_balanced_accuracy: 0.37941 |  0:00:48s\n",
      "epoch 73 | loss: 0.7875  | val_0_accuracy: 0.52273 | val_0_balanced_accuracy: 0.33068 |  0:00:48s\n",
      "epoch 74 | loss: 0.79184 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.34593 |  0:00:49s\n",
      "epoch 75 | loss: 0.76743 | val_0_accuracy: 0.44697 | val_0_balanced_accuracy: 0.34211 |  0:00:50s\n",
      "epoch 76 | loss: 0.78593 | val_0_accuracy: 0.46591 | val_0_balanced_accuracy: 0.30566 |  0:00:50s\n",
      "epoch 77 | loss: 0.79235 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.33443 |  0:00:51s\n",
      "epoch 78 | loss: 0.79011 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.32652 |  0:00:52s\n",
      "epoch 79 | loss: 0.78819 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.35989 |  0:00:52s\n",
      "epoch 80 | loss: 0.78267 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.35296 |  0:00:53s\n",
      "epoch 81 | loss: 0.77451 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.331   |  0:00:54s\n",
      "epoch 82 | loss: 0.79124 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.35296 |  0:00:54s\n",
      "epoch 83 | loss: 0.78857 | val_0_accuracy: 0.56061 | val_0_balanced_accuracy: 0.36947 |  0:00:55s\n",
      "epoch 84 | loss: 0.79859 | val_0_accuracy: 0.51136 | val_0_balanced_accuracy: 0.32813 |  0:00:56s\n",
      "epoch 85 | loss: 0.78038 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.33815 |  0:00:56s\n",
      "epoch 86 | loss: 0.77227 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.33654 |  0:00:57s\n",
      "epoch 87 | loss: 0.77295 | val_0_accuracy: 0.49621 | val_0_balanced_accuracy: 0.33187 |  0:00:58s\n",
      "epoch 88 | loss: 0.78194 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.32473 |  0:00:58s\n",
      "epoch 89 | loss: 0.78057 | val_0_accuracy: 0.51515 | val_0_balanced_accuracy: 0.32643 |  0:00:59s\n",
      "epoch 90 | loss: 0.772   | val_0_accuracy: 0.50379 | val_0_balanced_accuracy: 0.31556 |  0:00:59s\n",
      "epoch 91 | loss: 0.76763 | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.34464 |  0:01:00s\n",
      "epoch 92 | loss: 0.77416 | val_0_accuracy: 0.48106 | val_0_balanced_accuracy: 0.29382 |  0:01:00s\n",
      "epoch 93 | loss: 0.76913 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.32492 |  0:01:01s\n",
      "epoch 94 | loss: 0.76324 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.33859 |  0:01:02s\n",
      "epoch 95 | loss: 0.77682 | val_0_accuracy: 0.50758 | val_0_balanced_accuracy: 0.33815 |  0:01:02s\n",
      "epoch 96 | loss: 0.7763  | val_0_accuracy: 0.54167 | val_0_balanced_accuracy: 0.36959 |  0:01:03s\n",
      "epoch 97 | loss: 0.77273 | val_0_accuracy: 0.5     | val_0_balanced_accuracy: 0.35702 |  0:01:04s\n",
      "epoch 98 | loss: 0.76539 | val_0_accuracy: 0.51894 | val_0_balanced_accuracy: 0.33837 |  0:01:04s\n",
      "epoch 99 | loss: 0.78365 | val_0_accuracy: 0.53409 | val_0_balanced_accuracy: 0.33623 |  0:01:05s\n",
      "epoch 100| loss: 0.77365 | val_0_accuracy: 0.48864 | val_0_balanced_accuracy: 0.32973 |  0:01:06s\n",
      "epoch 101| loss: 0.76339 | val_0_accuracy: 0.49242 | val_0_balanced_accuracy: 0.33219 |  0:01:06s\n",
      "epoch 102| loss: 0.78053 | val_0_accuracy: 0.56818 | val_0_balanced_accuracy: 0.34943 |  0:01:07s\n",
      "epoch 103| loss: 0.76585 | val_0_accuracy: 0.55682 | val_0_balanced_accuracy: 0.36745 |  0:01:07s\n",
      "epoch 104| loss: 0.76337 | val_0_accuracy: 0.55303 | val_0_balanced_accuracy: 0.36105 |  0:01:08s\n",
      "epoch 105| loss: 0.76522 | val_0_accuracy: 0.54924 | val_0_balanced_accuracy: 0.36691 |  0:01:09s\n",
      "epoch 106| loss: 0.76689 | val_0_accuracy: 0.54545 | val_0_balanced_accuracy: 0.32769 |  0:01:09s\n",
      "epoch 107| loss: 0.76714 | val_0_accuracy: 0.56061 | val_0_balanced_accuracy: 0.34218 |  0:01:10s\n",
      "epoch 108| loss: 0.75935 | val_0_accuracy: 0.55682 | val_0_balanced_accuracy: 0.34133 |  0:01:10s\n",
      "epoch 109| loss: 0.7562  | val_0_accuracy: 0.54924 | val_0_balanced_accuracy: 0.33803 |  0:01:11s\n",
      "epoch 110| loss: 0.75131 | val_0_accuracy: 0.47348 | val_0_balanced_accuracy: 0.31247 |  0:01:12s\n",
      "epoch 111| loss: 0.75373 | val_0_accuracy: 0.53788 | val_0_balanced_accuracy: 0.40091 |  0:01:13s\n",
      "\n",
      "Early stopping occurred at epoch 111 with best_epoch = 36 and best_val_0_balanced_accuracy = 0.46007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение предсказанных классов: [250  43 116]\n",
      "Распределение истинных классов: [219 126  64]\n",
      "Accuracy: 0.4425\n",
      "F1-score: 0.4106\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          c0       0.56      0.64      0.60       219\n",
      "          c1       0.23      0.08      0.12       126\n",
      "          c2       0.26      0.47      0.33        64\n",
      "\n",
      "    accuracy                           0.44       409\n",
      "   macro avg       0.35      0.40      0.35       409\n",
      "weighted avg       0.41      0.44      0.41       409\n",
      "\n",
      "\n",
      "Топ-5 важных признаков:\n",
      " atr_norm     0.115579\n",
      "bb_width     0.059480\n",
      "ema_long     0.058632\n",
      "vwap_open    0.053979\n",
      "atr          0.048959\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "df = pd.read_csv('btc_usdt_4h_with_target.csv', index_col='timestamp', parse_dates=True)\n",
    "df.ffill(inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Пересчет целевой переменной\n",
    "df['high_next'] = df['high'].shift(-1)\n",
    "df['low_next'] = df['low'].shift(-1)\n",
    "df['u_t1'] = (df['high_next'] - df['close']) / df['close']\n",
    "df['v_t1'] = (df['low_next'] - df['close']) / df['close']\n",
    "threshold = 0.006\n",
    "df['target'] = np.where(df['u_t1'] >= threshold, 0, np.where(df['v_t1'] <= -threshold, 1, 2))\n",
    "\n",
    "# Добавление признаков\n",
    "def calculate_atr(df, period=14):\n",
    "    df['tr'] = np.maximum(df['high'] - df['low'], \n",
    "                         np.maximum(abs(df['high'] - df['close'].shift(1)), \n",
    "                                    abs(df['low'] - df['close'].shift(1))))\n",
    "    df['atr'] = df['tr'].rolling(window=period).mean()\n",
    "    return df\n",
    "\n",
    "df = calculate_atr(df)\n",
    "df['cmf'] = ta.cmf(df['high'], df['low'], df['close'], df['volume'], length=20)\n",
    "df['vwap'] = ta.vwap(df['high'], df['low'], df['close'], df['volume'])\n",
    "df['vwap_open'] = df['vwap'] / df['open']\n",
    "df['rsi'] = ta.rsi(df['close'], length=14)\n",
    "bb = ta.bbands(df['close'], length=20)\n",
    "df['bb_width'] = (bb['BBU_20_2.0'] - bb['BBL_20_2.0']) / df['close']\n",
    "df['volume_change'] = df['volume'].pct_change()\n",
    "macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "df['macd'] = macd['MACD_12_26_9']\n",
    "df['macd_signal'] = macd['MACDs_12_26_9']\n",
    "df['adx'] = ta.adx(df['high'], df['low'], df['close'], length=14)['ADX_14']\n",
    "df['ema_short'] = ta.ema(df['close'], length=12)\n",
    "df['ema_long'] = ta.ema(df['close'], length=50)\n",
    "df['atr_norm'] = df['atr'] / df['close']\n",
    "df['ema_ratio'] = df['ema_short'] / df['ema_long']\n",
    "stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3, smooth_k=3)\n",
    "df['stoch_d'] = stoch['STOCHd_14_3_3']\n",
    "df['obv'] = ta.obv(df['close'], df['volume'])\n",
    "df['cci'] = ta.cci(df['high'], df['low'], df['close'], length=20)\n",
    "df['roc'] = ta.roc(df['close'], length=10)\n",
    "df['atr_diff'] = df['atr'].diff(1)\n",
    "df['momentum'] = ta.mom(df['close'], length=10)\n",
    "\n",
    "# Разделение данных\n",
    "train_df = df['2017-08-23 16:00:00':'2021-12-31 16:00:00']\n",
    "val_df = df['2022-01-01 00:00:00':'2022-02-13 20:00:00']\n",
    "test_df = df['2022-02-14 00:00:00':'2022-04-23 00:00:00']\n",
    "\n",
    "# Ограничение признаков\n",
    "key_features = ['open', 'high', 'low', 'close', 'volume', 'close_open', 'high_low', \n",
    "                'open_diff_1', 'open_diff_2', 'close_diff_1', 'close_diff_2', \n",
    "                'volume_diff_1', 'volume_diff_2', 'cmf', 'vwap_open', 'atr', \n",
    "                'rsi', 'bb_width', 'volume_change', 'macd', 'macd_signal', 'adx',\n",
    "                'ema_short', 'ema_long', 'atr_norm', 'ema_ratio', 'stoch_d', \n",
    "                'obv', 'cci', 'roc', 'atr_diff', 'momentum']\n",
    "features = [col for col in df.columns if col in key_features]\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df['target'].values\n",
    "X_val = val_df[features].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df['target'].values\n",
    "\n",
    "# Обработка inf и NaN\n",
    "for i, col in enumerate(features):\n",
    "    max_val = np.percentile(df[col][~np.isinf(df[col])], 99)\n",
    "    min_val = np.percentile(df[col][~np.isinf(df[col])], 1)\n",
    "    X_train[:, i] = np.where(np.isinf(X_train[:, i]), max_val, X_train[:, i])\n",
    "    X_val[:, i] = np.where(np.isinf(X_val[:, i]), max_val, X_val[:, i])\n",
    "    X_test[:, i] = np.where(np.isinf(X_test[:, i]), max_val, X_test[:, i])\n",
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "# Нормализация и шум\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) + np.random.normal(0, 0.015, X_train.shape)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling с SMOTE\n",
    "smote = SMOTE(sampling_strategy={1: 3500, 2: 3500}, random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Проверка распределения\n",
    "print(\"Распределение y_train (после SMOTE):\", np.bincount(y_train_res))\n",
    "print(\"Распределение y_val:\", np.bincount(y_val))\n",
    "print(\"Распределение y_test:\", np.bincount(y_test))\n",
    "\n",
    "# Веса классов\n",
    "class_weights_dict = {0: 1.2, 1: 1.2, 2: 2.0}\n",
    "print(\"Веса классов:\", class_weights_dict)\n",
    "\n",
    "# Инициализация и обучение TabNet\n",
    "clf = TabNetClassifier(\n",
    "    n_d=128, n_a=128, n_steps=6, gamma=1.5,\n",
    "    lambda_sparse=0.0005, \n",
    "    optimizer_params=dict(lr=1e-3, weight_decay=1e-5),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={\"step_size\": 75, \"gamma\": 0.95},\n",
    "    mask_type='sparsemax', n_shared=2,\n",
    "    verbose=1, seed=42\n",
    ")\n",
    "clf.fit(\n",
    "    X_train_res, y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy', 'balanced_accuracy'],\n",
    "    max_epochs=300,\n",
    "    patience=75,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=512,\n",
    "    weights=class_weights_dict,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Предсказание и оценка\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Распределение предсказанных классов:\", np.bincount(y_pred))\n",
    "print(\"Распределение истинных классов:\", np.bincount(y_test))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['c0', 'c1', 'c2']))\n",
    "\n",
    "# Важность признаков\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=features)\n",
    "print(\"\\nТоп-5 важных признаков:\\n\", feature_importances.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567eba2-b14e-4ca9-863f-536a7cf4160f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
